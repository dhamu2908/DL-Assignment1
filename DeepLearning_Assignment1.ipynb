{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0Eydzb2wTkNPifS4N0kMn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhamu2908/DL-Assignment1/blob/main/DeepLearning_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fNHCJCimKeEB"
      },
      "outputs": [],
      "source": [
        "#importing packages\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Fashion MNIST dataset\n",
        "(train_data, train_labels), (test_data , test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Defining class names for Fashion MNIST\n",
        "fashion_classes = ['Ankle boot', 'Sandal', 'Trouser', 'Shirt', 'Coat', 'Bag', 'T-shirt/top', 'Dress', 'Pullover', 'Sneaker']\n",
        "\n",
        "# Function to select one image per class\n",
        "def select_images_per_class(images, labels, num_classes=10):\n",
        "    selected_images = []\n",
        "    for class_id in range(num_classes):\n",
        "        # Find the first image that belongs to the current class\n",
        "        first_image_index = np.where(labels == class_id)[0][0]\n",
        "        selected_images.append(images[first_image_index])\n",
        "    return selected_images\n",
        "\n",
        "# Selecting one image per class\n",
        "selected_images = select_images_per_class(train_data, train_labels)\n",
        "\n",
        "# Function to visualize and log images\n",
        "def visualize_and_log_images(images, class_names, save_filename='fashion_mnist_samples.png'):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for i in range(len(images)):\n",
        "        plt.subplot(2, 5, i + 1)\n",
        "        plt.imshow(images[i], cmap='gray')\n",
        "        plt.title(class_names[i])\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_filename)\n",
        "\n",
        "    # Log the image to Weights & Biases\n",
        "    wandb.login()\n",
        "    wandb.init(project='DeepLearning_Assignment1')\n",
        "    wandb.log({\"Fashion MNIST Samples\": wandb.Image(save_filename)})\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Visualize and log the selected images\n",
        "visualize_and_log_images(selected_images, fashion_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "Q5p1zcewKkfQ",
        "outputId": "77add69c-ecb6-4f17-8078-cecf057efe73"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 10 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA94AAAHICAYAAAC4fTKEAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAeRpJREFUeJzt3Xl4VFW2//9PGBJCJqYwQ4AwQyMKKDIPCipDO6CAE6gorTj11ba1R7222g6t0qgofbvRVmzUFlSUQVQcEQEBFZR5FCGEKQQCBMj5/eGPfA3Za5MqcpIg79fz9HOv69Sq2nXq7HPOpiprxQRBEAgAAAAAAISiXGkPAAAAAACAnzMW3gAAAAAAhIiFNwAAAAAAIWLhDQAAAABAiFh4AwAAAAAQIhbeAAAAAACEiIU3AAAAAAAhYuENAAAAAECIWHgDAAAAABAiFt4nYOTIkUpMTDzu43r16qVevXoV2+v26tVLbdu2LbbnA34Onn/+ecXExGj9+vUR544cOVKNGjUq9jEBAIpHTEyMbr755uM+7kSuBQAQplNu4f3MM88oJiZGZ511VmkP5aT04IMP6o033ijtYaCM+OabbzRkyBClpaWpUqVKqlevns4991yNGzeutIcGlIiYmJgi/e/DDz8s7aECZVZpXku4r0FZtmbNGo0ePVpNmjRRpUqVlJycrK5du2rs2LHav39/KK/58ssv68knnwzluU91FUp7ACVt0qRJatSokebPn6/Vq1eradOmpT2kk8qDDz6oIUOG6MILLyztoaCUzZ07V71791bDhg11/fXXq3bt2tq0aZPmzZunsWPH6pZbbintIQKhe/HFFwv897///W/Nnj27ULxVq1YlOSzgpFHc15KrrrpKw4YNU1xcXJEez30Nyqp33nlHl156qeLi4nT11Verbdu2ys3N1aeffqrf/OY3WrZsmSZMmFDsr/vyyy9r6dKluv3224v9uU91p9TCe926dZo7d66mTJmi0aNHa9KkSfrzn/9c2sMCTkoPPPCAUlJStGDBAlWpUqXAtm3btpXOoIASduWVVxb473nz5mn27NmF4sfKyclR5cqVwxxaKPbt26eEhITSHgZ+Ror7WlK+fHmVL1/e+5ggCHTgwAHFx8dH/PxASVi3bp2GDRumtLQ0ffDBB6pTp07+tjFjxmj16tV65513SnGEiMYp9VPzSZMmqWrVqhowYICGDBmiSZMmFXrM+vXrFRMTo8cee0wTJkxQenq64uLi1KlTJy1YsOC4r7FkyRKlpqaqV69e2rt3r/m4gwcP6s9//rOaNm2quLg4NWjQQHfddZcOHjxY5Pfz5ZdfqkuXLoqPj1fjxo317LPPFnrMtm3bdN1116lWrVqqVKmSTjvtNL3wwguFHrdv3z7dcccdatCggeLi4tSiRQs99thjCoIg/zExMTHat2+fXnjhhfyfT44cObLI48XPy5o1a9SmTZtCN0qSVLNmzfz/f+LEierTp49q1qypuLg4tW7dWuPHjy+U06hRIw0cOFCffvqpzjzzTFWqVElNmjTRv//970KPXbZsmfr06aP4+HjVr19ff/nLX5SXl1focW+++aYGDBigunXrKi4uTunp6br//vt15MiRE3vzQASO1uX48ssv1aNHD1WuXFm/+93vJBXtHP3hhx86f65+9Hr1/PPP58e2bt2qa665RvXr11dcXJzq1KmjX/7yl4X+3nXGjBnq3r27EhISlJSUpAEDBmjZsmUFHnO0jsmaNWt0wQUXKCkpSVdccUWx7RdAKvq15Kg33nhDbdu2VVxcnNq0aaOZM2cW2O76G++j15dZs2apY8eOio+P13PPPcd9DcqsRx55RHv37tU///nPAovuo5o2barbbrtNknT48GHdf//9+WuWRo0a6Xe/+12hNUVR7ol69eqld955Rxs2bMifE9TAKT6n1DfekyZN0sUXX6zY2FgNHz5c48eP14IFC9SpU6dCj3355ZeVnZ2t0aNHKyYmRo888oguvvhirV27VhUrVnQ+/4IFC9S/f3917NhRb775pvkvqXl5eRo8eLA+/fRT3XDDDWrVqpW++eYbPfHEE1q5cmWR/tZo165duuCCC3TZZZdp+PDhevXVV3XjjTcqNjZW1157rSRp//796tWrl1avXq2bb75ZjRs31muvvaaRI0dq9+7d+RM2CAINHjxYc+bM0XXXXaf27dtr1qxZ+s1vfqPNmzfriSeekPTjTypHjRqlM888UzfccIMkKT09/bhjxc9TWlqaPv/8cy1dutRb7G/8+PFq06aNBg8erAoVKmjatGm66aablJeXpzFjxhR47OrVqzVkyBBdd911GjFihP71r39p5MiR6tChg9q0aSPpx4VF7969dfjwYd19991KSEjQhAkTnPPt+eefV2Jiov7nf/5HiYmJ+uCDD/SnP/1Je/bs0aOPPlq8OwTw2LFjh84//3wNGzZMV155pWrVqlXkc3QkLrnkEi1btky33HKLGjVqpG3btmn27NnauHFj/s3Tiy++qBEjRqh///56+OGHlZOTo/Hjx6tbt25avHhxgZusw4cPq3///urWrZsee+yxk/JbepRtRb2WSNKnn36qKVOm6KabblJSUpL+/ve/65JLLtHGjRtVvXp1b+6KFSs0fPhwjR49Wtdff71atGjBfQ3KrGnTpqlJkybq0qXLcR87atQovfDCCxoyZIjuuOMOffHFF3rooYf03XffaerUqfmPK8o90e9//3tlZWXp+++/z7//L0ohaRRRcIpYuHBhICmYPXt2EARBkJeXF9SvXz+47bbbCjxu3bp1gaSgevXqwc6dO/Pjb775ZiApmDZtWn5sxIgRQUJCQhAEQfDpp58GycnJwYABA4IDBw4UeM6ePXsGPXv2zP/vF198MShXrlzwySefFHjcs88+G0gKPvvsM+976dmzZyAp+Nvf/pYfO3jwYNC+ffugZs2aQW5ubhAEQfDkk08GkoKXXnop/3G5ubnB2WefHSQmJgZ79uwJgiAI3njjjUBS8Je//KXA6wwZMiSIiYkJVq9enR9LSEgIRowY4R0fTg3vvvtuUL58+aB8+fLB2WefHdx1113BrFmz8o+/o3Jycgrl9u/fP2jSpEmBWFpaWiAp+Pjjj/Nj27ZtC+Li4oI77rgjP3b77bcHkoIvvviiwONSUlICScG6deu8rz169OigcuXKBebpiBEjgrS0tCK/d8AyZsyY4NhL69Fz9rPPPlsgXtRz9Jw5cwJJwZw5cwrkH71eTZw4MQiCINi1a1cgKXj00UfN8WVnZwdVqlQJrr/++gLxrVu3BikpKQXiI0aMCCQFd999d5HfPxCpol5LJAWxsbEF7km++uqrQFIwbty4/NjEiRMLXQuOXl9mzpxZ6PW5r0FZk5WVFUgKfvnLXx73sUuWLAkkBaNGjSoQv/POOwNJwQcffJAfK+o90YABA7gnCskp81PzSZMmqVatWurdu7ekH382PXToUE2ePNn5s9OhQ4eqatWq+f/dvXt3SdLatWsLPXbOnDnq37+/+vbtqylTphy3oMdrr72mVq1aqWXLltq+fXv+//r06ZP/fMdToUIFjR49Ov+/Y2NjNXr0aG3btk1ffvmlJGn69OmqXbu2hg8fnv+4ihUr6tZbb9XevXv10Ucf5T+ufPnyuvXWWwu8xh133KEgCDRjxozjjgennnPPPVeff/65Bg8erK+++kqPPPKI+vfvr3r16umtt97Kf9xPv4nOysrS9u3b1bNnT61du1ZZWVkFnrN169b5c02SUlNT1aJFiwLzbvr06ercubPOPPPMAo9z/QT2p6+dnZ2t7du3q3v37srJydHy5ctPbAcAEYiLi9M111xTIFbUc3RRxcfHKzY2Vh9++KF27drlfMzs2bO1e/duDR8+vMD1p3z58jrrrLOc158bb7wxonEAkSjqtUSSzjnnnALfSLdr107JycnOe7NjNW7cWP379y/28QPFbc+ePZKkpKSk4z52+vTpkqT/+Z//KRC/4447JKnA34FzT1T6TomF95EjRzR58mT17t1b69at0+rVq7V69WqdddZZysjI0Pvvv18op2HDhgX+++gi/NibmQMHDmjAgAE6/fTT9eqrryo2Nva441m1apWWLVum1NTUAv9r3ry5pKIVE6lbt26hAjdH84/+XdOGDRvUrFkzlStX8GM+Wl13w4YN+f+3bt26hSb4sY8DjtWpUydNmTJFu3bt0vz583XPPfcoOztbQ4YM0bfffitJ+uyzz3TOOecoISFBVapUUWpqav7ftx678D523kk/zr2fzrujx/WxWrRoUSi2bNkyXXTRRUpJSVFycrJSU1Pzi14d+9pAmOrVq1fo+lDUc3RRxcXF6eGHH9aMGTNUq1Yt9ejRQ4888oi2bt2a/5hVq1ZJkvr06VPoGvTuu+8Wuv5UqFBB9evXj2gcQKSKci2RinaNsDRu3LhYxwyEJTk5WdKPi+Pj2bBhg8qVK1eoS1Pt2rVVpUqVAtcR7olK3ynxN94ffPCBtmzZosmTJ2vy5MmFtk+aNEn9+vUrELMqYgY/KTYm/Xijc8EFF+jNN9/UzJkzNXDgwOOOJy8vT7/4xS/0+OOPO7c3aNDguM8BlCWxsbHq1KmTOnXqpObNm+uaa67Ra6+9piuvvFJ9+/ZVy5Yt9fjjj6tBgwaKjY3V9OnT9cQTTxQqiFbUeVcUu3fvVs+ePZWcnKz//d//VXp6uipVqqRFixbpt7/9rbMYGxCWE6meHBMT44y7fq11++23a9CgQXrjjTc0a9Ys/fGPf9RDDz2kDz74QKeffnr+cf/iiy+qdu3ahfIrVCh4WxAXF1foHwaAsFjXkqMdaE7kGkEFc5wskpOTVbduXS1durTIOdZ14ijuicqGU2LhPWnSJNWsWVNPP/10oW1TpkzR1KlT9eyzz0Z1Uo6JidGkSZP0y1/+UpdeeqlmzJihXr16eXPS09P11VdfqW/fvsedKJYffvihUFuXlStXSlJ+YZy0tDR9/fXXysvLK3DjdPTnJGlpafn/97333lN2dnaBb72PfdzR9wv4dOzYUZK0ZcsWTZs2TQcPHtRbb71V4JuKovw5hSUtLS3/W7ufWrFiRYH//vDDD7Vjxw5NmTJFPXr0yI+vW7cu6tcGilNRz9FHf3G1e/fuAvnWN+Lp6em64447dMcdd2jVqlVq3769/va3v+mll17K/5luzZo1dc455xT3WwKKzU+vJWHivgZl0cCBAzVhwgR9/vnnOvvss83HpaWlKS8vT6tWrcr/tZQkZWRkaPfu3fnXkUjuiZgT4fnZ/zP2/v37NWXKFA0cOFBDhgwp9L+bb75Z2dnZhf6OKBKxsbGaMmWKOnXqpEGDBmn+/Pnex1922WXavHmz/vGPfzjHu2/fvuO+5uHDh/Xcc8/l/3dubq6ee+45paamqkOHDpKkCy64QFu3btUrr7xSIG/cuHFKTExUz5498x935MgRPfXUUwVe44knnlBMTIzOP//8/FhCQkKhmz+cmubMmeP8luHo3xu1aNEi/9uJnz4uKytLEydOjPp1L7jgAs2bN6/APMvMzCzUHtD12rm5uXrmmWeifm2gOBX1HJ2Wlqby5cvr448/LpB/7LGck5OjAwcOFIilp6crKSkpv61M//79lZycrAcffFCHDh0qNKbMzMxieW9AURXlWhIm7mtQFt11111KSEjQqFGjlJGRUWj7mjVrNHbsWF1wwQWSpCeffLLA9qO/qh0wYICkyO6JEhIS+Ol5SH7233i/9dZbys7O1uDBg53bO3furNTUVE2aNElDhw6N+nXi4+P19ttvq0+fPjr//PP10UcfmW0xrrrqKr366qv61a9+pTlz5qhr1646cuSIli9frldffTW/z6RP3bp19fDDD2v9+vVq3ry5XnnlFS1ZskQTJkzIb3d2ww036LnnntPIkSP15ZdfqlGjRvrvf/+rzz77TE8++WT+t9uDBg1S79699fvf/17r16/XaaedpnfffVdvvvmmbr/99gKFTDp06KD33ntPjz/+uOrWravGjRvrrLPOinq/4eR1yy23KCcnRxdddJFatmyp3NxczZ07V6+88ooaNWqka665RhkZGYqNjdWgQYM0evRo7d27V//4xz9Us2bNqL/FuOuuu/Tiiy/qvPPO02233ZbfTuzot4dHdenSRVWrVtWIESN06623KiYmRi+++GJUP1sHwlDUc3RKSoouvfRSjRs3TjExMUpPT9fbb79d6O+xV65cqb59++qyyy5T69atVaFCBU2dOlUZGRkaNmyYpB9/wjh+/HhdddVVOuOMMzRs2DClpqZq48aNeuedd9S1a9dC/wgLhKko15IwcV+Dsig9PV0vv/yyhg4dqlatWunqq69W27Zt8+fH0daTt912m0aMGKEJEybk/5x8/vz5euGFF3ThhRfmF5WO5J6oQ4cOeuWVV/Q///M/6tSpkxITEzVo0KCS3gU/T6VUTb3EDBo0KKhUqVKwb98+8zEjR44MKlasGGzfvj2/PYurHYuk4M9//nP+f/+0ndhR27dvD1q3bh3Url07WLVqVRAEhduJBcGPLWMefvjhoE2bNkFcXFxQtWrVoEOHDsF9990XZGVled9Tz549gzZt2gQLFy4Mzj777KBSpUpBWlpa8NRTTxV6bEZGRnDNNdcENWrUCGJjY4Nf/OIX+a1nfio7Ozv49a9/HdStWzeoWLFi0KxZs+DRRx8N8vLyCjxu+fLlQY8ePYL4+PhAEi04TmEzZswIrr322qBly5ZBYmJiEBsbGzRt2jS45ZZbgoyMjPzHvfXWW0G7du2CSpUqBY0aNQoefvjh4F//+pez3cuAAQMKvY5r/nz99ddBz549g0qVKgX16tUL7r///uCf//xnoef87LPPgs6dOwfx8fFB3bp189vU6JjWTLQTQ3Gx2om1adPG+fiinqMzMzODSy65JKhcuXJQtWrVYPTo0cHSpUsLtBPbvn17MGbMmKBly5ZBQkJCkJKSEpx11lnBq6++Wuj55syZE/Tv3z9ISUkJKlWqFKSnpwcjR44MFi5cmP8Y1zUOKG5FvZZICsaMGVMoPy0trcC9iNVOzHV9CQLua1C2rVy5Mrj++uuDRo0aBbGxsUFSUlLQtWvXYNy4cfktwA4dOhTcd999QePGjYOKFSsGDRo0CO65555C7Y2Lek+0d+/e4PLLLw+qVKkSSOL+qBjFBAFf/wAAAAAAEJaf/d94AwAAAABQmlh4AwAAAAAQIhbeAAAAAACEiIU3AAAAAAAhYuENAAAAAECIWHgDAAAAABAiFt4AAAAAAISoQlEfGBMTE+Y4Tuj1S6oVecuWLZ3xp556ysx57bXXnPHFixebObm5uc74oUOHzJy2bds64xdddJGZs2bNGmf80UcfNXN2795tbjsZncixU9pzoqR07NjR3DZixAhnfMeOHWZOdna2M3748GEzp0aNGs647/PbuHGjM37aaaeZObVq1XLGU1NTzZzevXub205GJ8OcKAvXg5o1azrjffr0MXNGjRrljPvOq999950zbl0nJKlKlSrmti5dujjj8+bNM3N+97vfOeP79+83c6Jhfa4l9Zn6RDuGn9t1olGjRs54r169zJxf/vKXzrjvOvHSSy8544sWLTJzrHu0Sy65xMzp27evM56TkxPx2CZMmGDm/NycDNeJ4launPu7yry8vIifKzEx0dzWpk0bZ7x169ZmzjfffOOMHzhwwMypW7euuS0jI8MZ/+qrr8wcS1m4XpeUorwfvvEGAAAAACBELLwBAAAAAAgRC28AAAAAAELEwhsAAAAAgBCx8AYAAAAAIEQxQRFLyhVnFcKSqnDXvn17c9uwYcOccV/1yyNHjjjjCQkJZk58fLwzXr16dTOnOK1cudLcZlVibNGihZljVTqcNWuWmfPYY48540uXLjVzSsqpWJkzUr/5zW/MbRdccIEz7qvy2bhxY2c8KSnJzLGqmu/cudPMycrKcsZ9FaStKrtNmzY1c6z3c7IqS3OiOKtcW8eQJN12223O+DnnnGPmxMXFOeP79u2LOMeqxiz554XF1wHj+++/d8a3bNli5ljXMd/8+/jjj53xcePGmTm7du0yt5W2n2NV8/PPP98Z//Wvf23mWJXsY2NjzRyrsrLv2LY6tVidJyRp/fr1zrivY4Z13FvXD8mex/Xq1TNz3n//fWf81ltvNXPKsrJ0nSjLrPtp37HfqlUrZ7xDhw5mzieffOKM+87Rvm4t1py1usVI0pIlS8xtpwqqmgMAAAAAUMpYeAMAAAAAECIW3gAAAAAAhIiFNwAAAAAAIWLhDQAAAABAiFh4AwAAAAAQolJpJxaN5ORkc9u///1vZ7xdu3ZmTrly7n9zyM7ONnOs8vq+1i1WC7KKFSuaOSkpKc64r02N1b6pONuzSVKlSpWccavdjGS3GbHaH0jSVVddFdnAokRLjOO79957zW0NGjRwxn3t8qpVq+aMR7M/rePR93zRtBPr1q2bmdO1a1dn3GptU9aVpTkRTTux9PR0Z3zatGlmjtUm0TrnS/Z53zrnS9LBgwedcV/Ll8TExGJ7Hck+H/tay1SoUCGi5/Jty8nJMXOeffZZZ3zq1KlmTkk5WduJWfNBss/t1nyQpMqVKzvj1j2VZN+f+Np8WdcWH+t1fO0trbZhvrFZc983j61WY77r0Z133mluK21l6TpR2nxzrFGjRs74hg0bzJyLL77YGfe1Lp00aZIz7rsP8Y3buheqUqWKmWNdLxcuXGjm/NzQTgwAAAAAgFLGwhsAAAAAgBCx8AYAAAAAIEQsvAEAAAAACBELbwAAAAAAQuQuVVoGTZkyxdyWlpbmjG/bts3MsapcWtVbJbvKpa9Co/V8vpzt27c74+XLlzdzLL5Ko9HYv3+/M+6r/mtV+evRo4eZ07JlS2d8+fLlntEhDM2bNze3WZWQrUrMkpSQkOCMW9VyJSkzM9MZ980Jq3OAr0OCNV98XQis4/hkrWpelkRTOfehhx5yxrdu3WrmWNWIfZ+7NTZfNWTrvO+bL1aFct85Ny4uztxmzT9fdw7rPfnGYM0lXyX0MWPGOOOzZ882c/bu3Wtug3THHXeY26zzqo/1ufo6TFjHj2+urFu3zhm3qpD7xuCrau6bKxaro4Dv/tGqYt22bVszZ8CAAc74O++84xkdSpqv0rd13fF1nti0aZMz7uv2c9FFFznjvmPlvffeM7d99913zriv44G1FvN1PbLWFD9nfOMNAAAAAECIWHgDAAAAABAiFt4AAAAAAISIhTcAAAAAACFi4Q0AAAAAQIhYeAMAAAAAEKIy106sQ4cOzrhVpl6y22/5WjtYbYh8LTHq1avnjPvaIFmtN3ytW6xxWy0sJLtNja8djtXKIzs728z5/vvvI3ouH9/7GTVqlDN+5513Rvw6ODE1atQwtyUlJTnjVssiSUpJSXHGrZZOkj1ffe3yfGOwWK1lfG3LqlatGvHr4MTUqVPH3Fa7dm1n3NeGyGpx5TuvWed933FnHa++dkfWedJ3/vRdx6zx+Z7P2g++HKvNl68FmTW2QYMGmTn/+c9/zG2Qnn/+eXPbr3/9a2fc12bMaidkXQsk//2OJTc31xn3XY8se/bsMbcVZzsja8ySfd2zWkdJtA0rDb57iiZNmjjjvnaQ7du3d8Z9n/sPP/zgjKenp5s51hzztW+01jSS1KVLF2e8YcOGZo41PmvdINnnb1/OyY5vvAEAAAAACBELbwAAAAAAQsTCGwAAAACAELHwBgAAAAAgRCy8AQAAAAAIUZmrat67d29n3Ko27NvmqxRrVSk+ePCgmfPb3/7WGbcqEEp2Zb66deuaOVu2bHHGfdUWrWqavv1mVWI844wzzJxbbrnFGbcqy0t2lXbf5zNkyBBnnKrmJc+qxirZx6qv2nGbNm2ccV91cF8lZItvvlhycnKccatrgCS1bt064tfBifEdK1ZVc98xaVV+9VUotyp9+8651jnPd3z5tll8Vfit5/Odj60c3z5NTU11xn3XCutzOPfcc80cqpr7zZ8/39z2+eefO+ODBw82c7744gtn3NdFxuoAsGPHDjPHuqfxHT/WdcLXecYat68SunVs+1hjuPvuuyN+LoTHqlwuSQ0aNHDGfZXxV69e7Yy3a9fOzLHmrNVRQJIaNWrkjPfo0cPMWbBggbntzDPPdMZ91dg/+OADZ9x3nejataszvmLFCjNnyZIl5raTAd94AwAAAAAQIhbeAAAAAACEiIU3AAAAAAAhYuENAAAAAECIWHgDAAAAABAiFt4AAAAAAISozLUTs9pIWa1bJLt1iq+EfaVKlZzxrKwsM+cf//iHM96vXz8zx2rNNXHiRDNn9OjRzvjSpUvNnGrVqjnjvrYyVmuCJ554wsy56aabnHFfKxFrX1utmySpZcuWznjz5s3NnJUrV5rbcHxWG6SkpCQzxzomDx06FHFOlSpVzJz69es74752T1Y7GN9xZ7Wq8bWvqlOnjrkN4fC1YrHOeVabMcluPedrSWe1LvK1l1yzZo0zvn79ejNn3759Eb2+L0ey56bVykuy9/fAgQPNHGt8vnlutbj0zXNE7+9//7szftttt5k5GzdudMYzMzPNHOt49J2Ls7OzzW0Wa+775oN171KxYkUzxxqbr/XmjBkznHFf2zKUPN/5adu2bRHnBEHgjL/77rtmjnVMDBo0yMyZNWuWM+67hr3//vvmNmv95FtTVK9e3Rn3zT9rnvnuq6wWbXv37jVzyhK+8QYAAAAAIEQsvAEAAAAACBELbwAAAAAAQsTCGwAAAACAELHwBgAAAAAgRGWuqvlpp53mjG/atMnMsar2WVWafZKTkyPOmTlzprnNqubXunVrM+fOO+90xqdOnWrmWNUOfdXGFy1a5Ix36NDBzLGqy/uqzlrVEfPy8swcq3Lq2WefbeZQ1fzEWJXxfZUirSrgNWrUMHOseek7hqxjJT4+3syZO3duRM8l2ce3r4J0TEyMuQ3hmDx5srntk08+ccavuOIKM6dt27bO+IMPPmjmLF++3NwWqcqVK5vbrGPcd+z75pLVZcJXefY///mPM37PPfeYOQsWLHDGa9WqZeZYVa6bNGli5sDPdw9gne+6detm5jzwwAMRj8H6XH3daqzje//+/WaO9V59++DgwYPOuK8atMWXM23atIifD+Gxji9fdwfrePWdO61ze2pqqpljnaM3bNhg5ljVwb/44gszx9eBw1qj+Oasdfz77pGsuembS1aXm+K8JoeJb7wBAAAAAAgRC28AAAAAAELEwhsAAAAAgBCx8AYAAAAAIEQsvAEAAAAACBELbwAAAAAAQlQq7cSs1i2SlJmZ6Yz7StiXL1/eGfeVsLdaCezYscPMsfjej9Wqok6dOmaO1a7D934OHToUcY6vNZfFaj9Qr149MyeadmJWy5Du3bubOS+88IK5DcdXtWpVZ9w6hiX7M/S15LCez5rHktSmTRtnfPPmzWZOw4YNnfH169ebOVbbsD179pg51txDeB555BFzm3VMzpkzx8xZvHixM+5rL2m1LvGdc63jyHfd2b17tzPuO+6CIDC3WeNLSUkxc6z5t2bNGjPHat/ma09o7QffOQh+vnsny5YtW8xt1mfeuHFjM8c6r2ZnZ5s51jz2tXa0WhD5jjmrrVM0bZN87Z5QtlgtT33nb+vYs9p/SdLOnTudcV+7Y2t9UqVKFTNn1KhREb2+5G/taO0H37nYag3mm0tWG9vc3Fwzxxo37cQAAAAAAAALbwAAAAAAwsTCGwAAAACAELHwBgAAAAAgRCy8AQAAAAAIUalUNf/tb39rbrOq+fmqUlpVs63nkuzqhL7qex07dnTGq1evbuZYFfsqVqxo5lgV+3xVbK3346subVVIHDp0qJljVb62qpBLdrVcX441buszwImzKnPm5ORE/Fy+4zspKckZ3759u5ljVWm2Kj5L9pxIS0szc6yqyr7zgu+9IhyzZs0yt/Xt29cZv+SSS8ycfv36OeO+Tgk33nijM+6rPNu0aVNnPDEx0cyxjn1fFwDfed+qFuvrMvHSSy85476q1NZ13letdteuXc74xRdfbOZ06dLFGfdV80X0rIre1nldso8tX2VnqwOA79i2zvm+Y84STTX4bdu2RZyD0mEde751g3We9t2HJCQkOOPWukWyj2PfvdjgwYOd8Y8++sjM8XV4sa5jVuVyyb4mVa5c2cyxOjwtWbLEzKldu7a57WTAN94AAAAAAISIhTcAAAAAACFi4Q0AAAAAQIhYeAMAAAAAECIW3gAAAAAAhIiFNwAAAAAAISqVdmJz5841t1ll4q02LJKUnJzsjFtl/CVp1apVzrivxP+8efOccV8bFmub73Wskvy+Mv4xMTERv47VFsTXImblypXOuK9dgPV+rNeXpB9++MEZf+ONN8wcnBjrWPW1fbP4PtusrCxnvFWrVhG/jtV+SLJbEFpzX5IaNmzojPva3vjmC8Lx17/+1dxmtV20zimS9N133znjgwYNMnP+9Kc/mdss1tgOHjxo5ljncKvNmORvhWSdj31t8aw2Or75N3/+fGd869atZs6cOXOccd+cpW1Y9KzztO+e5vvvv3fG27VrF/Hr+I576/j2HafWXLFaZUr29c1q6SRJNWrUcMY3b95s5lh893XRtDRD0Vjtsnyti62Web4c6xj3HZMWX6uz999/3xnftGmTmeMbg3X8+3Kstn2++yerRZrv3GCNwVoHSf7rZUnjG28AAAAAAELEwhsAAAAAgBCx8AYAAAAAIEQsvAEAAAAACBELbwAAAAAAQlQqVc3Hjx8f8baqVauaOc2aNXPGb7zxRjOnZ8+ezrivQurSpUud8d27d5s5VgVOq7JscfNV+bMqjfqqeaakpDjjX3/9tZlzxRVXmNtQdkRT4dbiy7EqXFoVQ33WrFljbjvttNOccasyvyTt27fPGbeOe8nfOQDhmDJlirmtb9++znjHjh3NnBkzZjjjb731lplTs2ZNZ3zjxo1mTjQVxa0Krr5qyD5WpWSruqxkV6u1OopIUlpamjN+++23R5zTq1cvM2fx4sXO+JIlS8wcRG/9+vXOuK+TRWxsrDPuu6+zXsdX6bt69erOuK/6vvV8vqrK1nulCnnZ4uu2Y91vWJ0nJKlJkybOuO/caa0Poqmy7btOWN1VfPPStw6JpruSdd/nq8ZudQjw7R/rc7XmvyRt377d3FbS+MYbAAAAAIAQsfAGAAAAACBELLwBAAAAAAgRC28AAAAAAELEwhsAAAAAgBCx8AYAAAAAIESl0k4sGr52EPPnz3fGfe0g+vTp44z7SthbLTESEhLMHKskfzQtmnytwaxt0bR1slrHSHZrm7lz55o5ODlYx4qvRYrVRmP//v1mjtU+wteSw+JrDdalSxdn3NcuLyMjwxmvW7eumVNSrQHx/7Ru3drcZh17W7duNXPmzZvnjHft2tXMadu2rTPuu4ZEc6xY89L3OtFcK3xjs8bg26cvv/yyM+5r87V27VpnfNOmTWaO7xyA4mfNr+JuO2kdj9Y9iO/5fPeP1vUomvaWvnZPKHm+c6R1rPju5632ib61RjSsll2+92O17PLdi/kkJiY6475ri9WKrXnz5mZOvXr1nHHfXLLuFWvVqmXm0E4MAAAAAIBTBAtvAAAAAABCxMIbAAAAAIAQsfAGAAAAACBELLwBAAAAAAhRmatqblXM81W4s6pw+yoA7tmzxxn3VXY9cuRIxK9j8VUGjOb5ilM0lXd3795drK8TTSVfhMO3z63qm77K4dZcjuYYWrZsWcQ5VhVbyZ6XmZmZZg7HZMlr0qSJuc06JuvXr2/mWNW5fZX2rWr/2dnZZk65cu5/6/Z1DrDOk9b1KFq+ar5WtdrU1FQzx9p3vmrR1mdUpUoVM6d27drOuFUhHf9PNJXIrWPVd4607tF81cYtvhzrdayKz5K0bds2Z9x3bO/du9fchrLDt27Yt29fxDnWfc2OHTvMnOrVqzvj0dxX+dYN1jHpq2rue6/WOd8am4/v2mJVG/fdD1qdDXzzvCzhG28AAAAAAELEwhsAAAAAgBCx8AYAAAAAIEQsvAEAAAAACBELbwAAAAAAQsTCGwAAAACAEJW5dmJWiX2rtL3PmjVrzG1WOzFfqXyrVYWP9X6Ku52Y7/ks1vvxtRiwWPvTx2qtIxV/qxwcn/V5xMbGmjlWWwffXLFa2ETTomXhwoXmNuv9RNPGLi4uzszxtZxCOHznDqvli++cYrUAq1y5spljHSu+48va5jt/W+/Vtw98z2eN2/d81jnA916tNjE+1apVc8Z91+W6des647QTOz7rM/e1GbPawVWtWtXMsc6R1uft4zuurPmakpJi5kRzX2fNr7S0tIify9dKECfGd/62zim++2+rNZjv/iCaNYB1TPjOt9Zc9s0xq6WaJB08eNAZ9+1Ta3y+Y7xWrVrOuNUmUrLbt1n3o2UN33gDAAAAABAiFt4AAAAAAISIhTcAAAAAACFi4Q0AAAAAQIhYeAMAAAAAEKIyV9XcEk0F7P3795s5ViVLX3VCqzKfr+KqVbnQVznRyomm8q3vdaKpWmiNgcqcJz/rGPIdd9ax76twaz3ft99+6xmd2+7duyPO8c0JX9XQaJ4P4YimorevUvPOnTud8fj4eDMnmurg0RwrVk401xDJ7hDiu/ZZ89z3Xrdu3eqMW1XnJfta7puXVpVtHJ9vTlgyMzOd8aVLl5o5mzZtcsZ99xrWcWJVQZbs+7r169dH/Dq+Suhbtmxxxq0K+ygdVhVyKbpODb4q4Bbr3tjXPcg6D1r37D7RdACR7POqrzuI9Z58nQisa2w0427QoIGZU5bwjTcAAAAAACFi4Q0AAAAAQIhYeAMAAAAAECIW3gAAAAAAhIiFNwAAAAAAIWLhDQAAAABAiE6admLRtGHxtcqwSuL7Xsfa5it7b/GNLZqWRlb7mGha2/jGFk3bskhfH6XDaq/ha6lktYnwtVWx2hZZLWd8srOzzW3RtP+zjm9fuzyrhQ1Kh3X+9J3XMjIynHHfsR+NaFqdRdPKK5p2a742MdFck6KZF9a4i3tsiF737t2d8bVr15o5GzZscMZ97Yz27NnjjCcnJ5s5VguwaFrL1qlTx8yx1K5d29xWs2ZNZ3zbtm1mjjUfomkDdyryteyy7neaNWtm5ljnGqt1oiS1bdvWGd+7d6+ZU6lSJXObJZpjwteezLqH27Vrl5nTqVMnZzwrK8vMsa69vraB1jWsRo0aZk5ZwjfeAAAAAACEiIU3AAAAAAAhYuENAAAAAECIWHgDAAAAABAiFt4AAAAAAITopKlqXtzq1avnjPsq9lkVDX3Vua2qlFZVvpJkje3QoUNmjjVuKsv+fPkqRebk5DjjVsVQya40unr16sgGdhxWxXPf2Kzqt74qo/v27YtsYDhh0XRE8J1zrfO+rypuNF0hrDH4quZH00kimv0TzRh8+9SqCL97924zJ5pqvtHknEp8Fe6tY7VBgwZmTuvWrZ1xX1XzKlWqOOO+a4t1PUhISDBzGjdu7Iz7jjlflfRI+SpVX3755c74k08+aeZQvfzE+M5p1j2r7/5gx44dEedY50HfsWJJTEw0t1nV+X05VhcA3/P55lKjRo2c8W+//dbM+eKLL5zx888/38z55ptvnHHf9ahly5bO+PLly82csPCNNwAAAAAAIWLhDQAAAABAiFh4AwAAAAAQIhbeAAAAAACEiIU3AAAAAAAhYuENAAAAAECITpp2YtG0R/HxtRmwWC0Djhw5YuZY5e19Ze+tbb59YOX42lFYrXIOHjxo5lhj8LXdifS5UDqs47hy5cpmTv369Z1x3/FgzaMVK1Z4Rhe5nTt3OuNWaxvJbvFR3K2bcHLwtaqyzq2+c3txtpeM9riz8nzPZ7WW8bWqstro+NoGtm/fPqLXl8pGa86yLJqWVP379ze3Wa2BfHNlz549zrjVfkiSNm/e7IxbbYEk+71+//33Zk67du2c8YyMDDOnevXqzrivHa3VwrZp06ZmTnG32DzV+I5J637Hl/PJJ5844745ZrVcjaYFr2/dYo2hQoXolnlWm1Tf/VM0x6vVos2KS/b1wHcN87UuLGl84w0AAAAAQIhYeAMAAAAAECIW3gAAAAAAhIiFNwAAAAAAIWLhDQAAAABAiE6aqubFzarc7as0aFUU9OVYlQZ91fes5/NVdrWez1fR0MqxqjD6+Cod4ucrISEh4hyrCrGvImw0rEq2rVq1MnOs84KvSrtvXiIc2dnZ5jbrmPRV4LZYlbml6CqrRlNh2nq+aDpjSPb1xTfuQ4cORfw61v7euHGjmdOxY0dn3NdpI5rqwPCzKn1L0tdff+2M+z4Hq5NFXFxcZAM7zutYfPPO2nbgwAEzp0GDBs64Vb3dt81X2Z2q5ifGOm9JdgcT3+durQGiubb4WPNi9+7dZo71Xn1V2rOyssxtVsca33tdu3atM163bl0zJzMz0xn33Vta55NNmzaZOdbnXRr4xhsAAAAAgBCx8AYAAAAAIEQsvAEAAAAACBELbwAAAAAAQsTCGwAAAACAELHwBgAAAAAgRKdsO7Fo2rpYfC1VfC1aLFa5ft/rRPP61vP5cqx2Cr62O5Zo9g1Knq99ROXKlSOKS3YbpuJuJ7Zt2zZnvGXLlmaO1RbP1y5v8+bNkQwLEbDahvjOHdbx6mv1Y/G1kfO1qrFY47bepyQdOXLEGY/meiDZLSat15Gia4tpvc769evNHGt/+8bm+4zgZ7Wy2rJli5ljtSfyteuxjgXrfkKK7p7Cej7f/V40Lc2slqu1atUyc6zrRGpqasSvj6KJpq2i7zphHeO+1lfRnL+t49jXHtja5jv2fa35rOfztTSz5lLNmjXNHOvaN3/+fDPH2t/79+83c2gnBgAAAADAKYKFNwAAAAAAIWLhDQAAAABAiFh4AwAAAAAQIhbeAAAAAACE6JStau6r1Byp4q7OXVJVza3Xiaaqua+KNU5uvsqz1jHkOx6ys7OdcavaebR27NgR8etY79VXddq3f3BirHNRNNW0o6k+76v6ao0hmo4ZvnO7tc2X4xuDVWU3mvfqqzaelJTkjK9cudLMiaYyb7TV3SE1bNjQGfftb+sz8p0jrUrovuPHV8HZUrVqVWfcd462Xsf3+uvWrXPGmzVrZuZkZGQ44ykpKWZOtWrVnPGdO3eaOfh/ovncfcfx9u3bnfGOHTtGNrDjOHjwoDPuO0dHc/9knaMl6cCBA864r4K7xVdRvEGDBs647zrRo0cPZ9zab5K/M01J4xtvAAAAAABCxMIbAAAAAIAQsfAGAAAAACBELLwBAAAAAAgRC28AAAAAAELEwhsAAAAAgBCdNO3Eirtll8VXrj8a1rijaYESzdii2W++VmvRtKLBycFq+bJv3z4zxzqOfe3Efvjhh8gGFqX169c74xUrVjRzrBYaPocOHYo4BycmmjaJ0bQT850LrTH4ji/r+Xznz+JuTxZNa7BorldWm6Rly5aZOdb+8X0OtBOLnnXc+fZ3Tk6OM+4751tzwtcCyTrufXM/MTHRGfe1lbJaENWrV8/MWbhwoTNutTmSpC1btjjjvrZlVns02omFJ5p7gP3795vbrGPf97lbx6vvHG1t812PfPcu1nz2tRPLyspyxvfs2WPmWOPbvXu3mWOdt3znhmg+17DwjTcAAAAAACFi4Q0AAAAAQIhYeAMAAAAAECIW3gAAAAAAhIiFNwAAAAAAITppqppHU6XVx6qm6avMGQ2rMqeviq1V0bC490E0irOqeUmNGUVjVZf0VYSNjY2NKC75q1UWp23btjnjvuMumkrV0VSdxomJpqr5xo0bI34dq+KxJGVmZjrj2dnZZo5vLlmsc67vehBNFXDf88XFxTnjVicEya5+66sub43BN8d81YHhV6NGDWfcd/62jvu2bduaOdZx4qt2bI3BN4eSkpIiei7Jrnbcrl07M+edd95xxn3XNmsMVuVyiWM7TNY5zXedSE5OdsbbtGlj5nz99dfOuO/cad1P+44HK8dXudx3fYuPj484x7pW+cZg7YdorpW+nLI0l/jGGwAAAACAELHwBgAAAAAgRCy8AQAAAAAIEQtvAAAAAABCxMIbAAAAAIAQsfAGAAAAACBEZae+ehnha8NSnG1dfK9jbfO1VPGNwWK15PGNzRJNOzGULdG0E7P4jqH9+/dH/HzW8e1rK2W1ibFaCUr2HPe1vbFeBycumtZXFt9naLFazvi2+VqnVKtWzRm3jjspuvaSPlaeb85a+85qGSZJdevWdcZ988VqueRrBeNrFQU/q52Y71jYsWOHM56SkmLmWJ/fli1bzBzrc921a5eZs2/fPmc8mnsan7179zrjvrFZ92/WmCWpTp06zviKFSs8o8NRVvsvSWrQoIEzvmTJEjOnYcOGznijRo3MnK+++soZ953TrHO+7z7buob88MMPZk716tUjfj7f8WqdA3JycsycmjVrOuO+ezvrGmudzyT/Nbak8Y03AAAAAAAhYuENAAAAAECIWHgDAAAAABAiFt4AAAAAAISIhTcAAAAAACE6aaqa+yrcRcOq9Ne8eXMzx6o06Ks2bm2zKkj7cnyvY+0fXyU/X1XFSF8nmqrmxf2ZIhy+Sq0WX+XLaKqaW1Vpfcf39u3bnXFflXZrjkVTPR0nzjqv+CrTW59vNJWNX3/9dXObVTF327ZtZo51zo2mc4Dv/O2reG5t811frPFlZWWZOQsXLjS3Rfo6vv1T3BWrTyWJiYnOuK8KcdWqVSN+nUqVKjnjvnlsHd+pqalmTmZmpjPuq75vPZ+vQnJ6eroz7ptD0XSrSUpKMrfh+JYuXWpuW7dunTPuO6dZVcDffPNNMyc+Pt7cZonmenDw4MGI4pJUpUoVc1t2drYz7ptL1v2Y757P2qe+bhVTp051xn3zxddtpKRxxQIAAAAAIEQsvAEAAAAACBELbwAAAAAAQsTCGwAAAACAELHwBgAAAAAgRCy8AQAAAAAI0UnTTqy4WWX0faXyrfYWvrYTVgsJXwsUX6uxSPnaLVmtejZt2mTmVK5c2Rm32mv4+PaBr8UGwmG1VfG1b9mxY4czbrWPkaJrvxVNOzGrJUdcXJyZY7UN87W2sFry4MRZrVh87bKsY8XXOsXy0EMPRZyD6Fnzz3etiOZzxY+aNWvmjFutliT/ud1ifX7W/YRkXyfmzp1r5lx++eXOuK/13vvvv++M+465aM4xVotN376eM2eOuQ3Ht2fPnqi2Wc4444yIc6K53/GtQyzWvZCvxZbvPtsaQzTz33ePZM3Nhg0bmjmrV692xq0WaGUN33gDAAAAABAiFt4AAAAAAISIhTcAAAAAACFi4Q0AAAAAQIhYeAMAAAAAEKKTpqq5r4qtVQnVZ/Hixc74t99+a+bs3r3bGY+mCrmvYubevXudcd/7tPaPVdlZsisa5ubmmjlVq1Z1xufPn2/mRPr6KB1ff/21Mz5t2jQzxzr2d+7caeZEU6k1mmNl69atzviqVavMHOv43rZtm5mzdOnSyAaGIrOOo5UrV5o533//vTP+xRdfRPz6vuuOJZrrEX40adIkZ7xJkyZmzqJFi8Iazs/eTTfd5Iz77huse5dXXnnFzLG6nmzYsMHMqV+/vjO+fv16M2fhwoXmtki9/vrrEee89tprxfb6KB2+CvhWhXJf5XKrCrgvx7qG+OalNW7f6/ier2bNms64717Iql7uqyC/f//+iHMsJ0unJL7xBgAAAAAgRCy8AQAAAAAIEQtvAAAAAABCxMIbAAAAAIAQsfAGAAAAACBELLwBAAAAAAhRTEDvEwAAAAAAQsM33gAAAAAAhIiFNwAAAAAAIWLhDQAAAABAiFh4AwAAAAAQIhbeAAAAAACEiIU3AAAAAAAhYuENAAAAAECIWHgDAAAAABAiFt4AAAAAAISIhTcAAAAAACFi4Q0AAAAAQIhYeAMAAAAAECIW3gAAAAAAhIiFNwAAAAAAIWLhDQAAAABAiFh4AwAAAAAQIhbeAAAAAACEiIU3AAAAAAAhYuENAAAAAECIWHgDAAAAABAiFt4AAAAAAISIhTcAAAAAACFi4Q0AAAAAQIhYeAMAAAAAECIW3gAAAAAAhIiFNwAAAAAAIWLhDQAAAABAiFh4AwjN+vXrFRMTo8cee+y4j7333nsVExNTAqMCAJR1MTExuvfee/P/+/nnn1dMTIzWr19famMCfi4iuT9D8WHhHaKjF4mf/q9mzZrq3bu3ZsyYUdrDAwodn9b/Pvzww9IeagE5OTm69957vePatWuXKlSooFdffVWS9OCDD+qNN94omQEC/79jrwOVKlVS3bp11b9/f/39739XdnZ2aQ8RKBauY7158+a6+eablZGRUdrDA0rcN998oyFDhigtLU2VKlVSvXr1dO6552rcuHGlPTSUkgqlPYBTwf/+7/+qcePGCoJAGRkZev7553XBBRdo2rRpGjhwYGkPD6ewF198scB///vf/9bs2bMLxVu1ahX6WP7whz/o7rvvLtJjc3JydN9990mSevXq5XzMrFmzFBMTo379+kn6ceE9ZMgQXXjhhcUxXCAiR68Dhw4d0tatW/Xhhx/q9ttv1+OPP6633npL7dq1K+0hAsXi6LF+4MABffrppxo/frymT5+upUuXqnLlyqU9PKBEzJ07V71791bDhg11/fXXq3bt2tq0aZPmzZunsWPH6pZbbintIaIUsPAuAeeff746duyY/9/XXXedatWqpf/85z8svFGqrrzyygL/PW/ePM2ePbtQvCRUqFBBFSr4T0l5eXnKzc0t0vNNnz5dXbt2VZUqVYphdMCJOfY6cM899+iDDz7QwIEDNXjwYH333XeKj4935u7bt08JCQklNVTghPz0WB81apSqV6+uxx9/XG+++aaGDx9eyqMLD/MUP/XAAw8oJSVFCxYsKHQfsm3bttIZVAnLycnhH9uOwU/NS0GVKlUUHx9fYJHx2GOPqUuXLqpevbri4+PVoUMH/fe//y2Uu3//ft16662qUaOGkpKSNHjwYG3evLnQ30IBJWHhwoXq37+/atSoofj4eDVu3FjXXnut87ETJkxQenq64uLi1KlTJy1YsKDAdtffeMfExOjmm2/WpEmT1KZNG8XFxenZZ59VamqqJOm+++7L/1njT4//vLw8zZw5UwMGDMh/nn379umFF17If/zIkSPzH7948WKdf/75Sk5OVmJiovr27at58+YVGMvRn1F+/PHHGj16tKpXr67k5GRdffXV2rVrV7S7EKewPn366I9//KM2bNigl156SZI0cuRIJSYmas2aNbrggguUlJSkK664QtKPx/WTTz6pNm3aqFKlSqpVq5ZGjx5d6PgryrycPHmyOnTooKSkJCUnJ+sXv/iFxo4dWzJvHKeUPn36SJLWrVunXr16OX+lNHLkSDVq1Ciq53/mmWfyrw9169bVmDFjtHv37vztN998sxITE5WTk1Mod/jw4apdu7aOHDmSH5sxY4a6d++uhIQEJSUlacCAAVq2bFmh8VrzFJCkNWvWqE2bNs5//K9Zs2b+/3/0PueNN95Q27ZtFRcXpzZt2mjmzJmF8jZv3qxrr71WtWrVyn/cv/71rwKPyc3N1Z/+9Cd16NBBKSkpSkhIUPfu3TVnzpzjjjkIAt1www2KjY3VlClT8uMvvfSSOnTooPj4eFWrVk3Dhg3Tpk2bCuT26tVLbdu21ZdffqkePXqocuXK+t3vfnfc1zzV8I13CcjKytL27dsVBIG2bdumcePGae/evQW+VRw7dqwGDx6sK664Qrm5uZo8ebIuvfRSvf322/mLB+nHk/2rr76qq666Sp07d9ZHH31UYDtQUrZt26Z+/fopNTVVd999t6pUqaL169cXOFkf9fLLLys7O1ujR49WTEyMHnnkEV188cVau3atKlas6H2dDz74QK+++qpuvvlm1ahRQ6eddprGjx+vG2+8URdddJEuvvhiSSrwU90FCxYoMzNTF1xwgaQff1I/atQonXnmmbrhhhskSenp6ZKkZcuWqXv37kpOTtZdd92lihUr6rnnnlOvXr300Ucf6ayzziownptvvllVqlTRvffeqxUrVmj8+PHasGGDPvzwQ4rDIWJXXXWVfve73+ndd9/V9ddfL0k6fPiw+vfvr27duumxxx7L/8Zg9OjRev7553XNNdfo1ltv1bp16/TUU09p8eLF+uyzz1SxYsUizcvZs2dr+PDh6tu3rx5++GFJ0nfffafPPvtMt912W8nvBPysrVmzRpJUvXr1Yn/ue++9V/fdd5/OOecc3Xjjjfnn5AULFuTPiaFDh+rpp5/WO++8o0svvTQ/NycnR9OmTdPIkSNVvnx5ST9eK0aMGKH+/fvr4YcfVk5OjsaPH69u3bpp8eLFBf5xwJqngCSlpaXp888/19KlS9W2bVvvYz/99FNNmTJFN910k5KSkvT3v/9dl1xyiTZu3Jg/bzIyMtS5c+f8hXpqaqpmzJih6667Tnv27NHtt98uSdqzZ4/+7//+T8OHD9f111+v7Oxs/fOf/1T//v01f/58tW/f3jmGI0eO6Nprr9Urr7yiqVOn5q8tHnjgAf3xj3/UZZddplGjRikzM1Pjxo1Tjx49tHjx4gL/sLBjxw6df/75GjZsmK688krVqlXrhPfjz06A0EycODGQVOh/cXFxwfPPP1/gsTk5OQX+Ozc3N2jbtm3Qp0+f/NiXX34ZSApuv/32Ao8dOXJkICn485//HNp7walhzJgxQVFPC1OnTg0kBQsWLDAfs27dukBSUL169WDnzp358TfffDOQFEybNi0/9uc//7nQa0sKypUrFyxbtqxAPDMz03vM//GPfwzS0tIKxBISEoIRI0YUeuyFF14YxMbGBmvWrMmP/fDDD0FSUlLQo0eP/NjR+dyhQ4cgNzc3P/7II48EkoI333zT3A84dR09bnzzJCUlJTj99NODIAiCESNGBJKCu+++u8BjPvnkk0BSMGnSpALxmTNnFogXZV7edtttQXJycnD48OFo3xZQyNFj/b333gsyMzODTZs2BZMnTw6qV68exMfHB99//33Qs2fPoGfPnoVyR4wYUeicfew5/ujzr1u3LgiCINi2bVsQGxsb9OvXLzhy5Ej+45566qlAUvCvf/0rCIIgyMvLC+rVqxdccsklBZ7/1VdfDSQFH3/8cRAEQZCdnR1UqVIluP766ws8buvWrUFKSkqBuDVPgaPefffdoHz58kH58uWDs88+O7jrrruCWbNmFbh/CIIfj/PY2Nhg9erV+bGvvvoqkBSMGzcuP3bdddcFderUCbZv314gf9iwYUFKSkr+OuLw4cPBwYMHCzxm165dQa1atYJrr702P3b0/uzRRx8NDh06FAwdOjSIj48PZs2alf+Y9evXB+XLlw8eeOCBAs/3zTffBBUqVCgQ79mzZyApePbZZyPdVacUfmpeAp5++mnNnj1bs2fP1ksvvaTevXtr1KhRBb6B+Onf9u3atUtZWVnq3r27Fi1alB8/+rOTm266qcDzU6ABpeHov3K+/fbbOnTokPexQ4cOVdWqVfP/u3v37pKktWvXHvd1evbsqdatW0c0tunTpxfplyBHjhzRu+++qwsvvFBNmjTJj9epU0eXX365Pv30U+3Zs6dAzg033FDgW/obb7xRFSpU0PTp0yMaI3BUYmJioermN954Y4H/fu2115SSkqJzzz1X27dvz/9fhw4dlJiYmP8zwqLMyypVqmjfvn2aPXt28b8ZnPLOOeccpaamqkGDBho2bJgSExM1depU1atXr1hf57333lNubq5uv/12lSv3/25nr7/+eiUnJ+udd96R9ONPeS+99FJNnz5de/fuzX/cK6+8onr16qlbt26SfvwlyO7duzV8+PACc6x8+fI666yznD/VPXaeAkede+65+vzzzzV48GB99dVXeuSRR9S/f3/Vq1dPb731VoHHnnPOOfm/wpN+/AVfcnJy/j1SEAR6/fXXNWjQIAVBUOD47N+/v7KysvLXC+XLl1dsbKykH/88aefOnTp8+LA6duxYYE1xVG5ubv4vbKdPn55fkFaSpkyZory8PF122WUFXrN27dpq1qxZoTkRFxena665pnh24M8UPzUvAWeeeWaBojrDhw/X6aefrptvvlkDBw5UbGys3n77bf3lL3/RkiVLdPDgwfzH/vSnqxs2bFC5cuXUuHHjAs/ftGnT8N8ETll79+4tcLNSvnx5paamqmfPnrrkkkt033336YknnlCvXr104YUX6vLLL1dcXFyB52jYsGGB/z66CC/K30Yfe7wfz9atW7Vo0SL97//+73Efm5mZqZycHLVo0aLQtlatWikvL0+bNm1SmzZt8uPNmjUr8LjExETVqVOH3rKI2t69ewv8zV+FChVUv379Ao9ZtWqVsrKyCjzup44W6ynKvLzpppv06quv6vzzz1e9evXUr18/XXbZZTrvvPNCeoc4lTz99NNq3ry5KlSooFq1aqlFixYFFsbFZcOGDZJU6PwdGxurJk2a5G+XfvzH3yeffFJvvfWWLr/8cu3du1fTp0/P//Mn6cc5Jv2/v0k/VnJycoH/ds1T4Kc6deqkKVOmKDc3V1999ZWmTp2qJ554QkOGDNGSJUvyv1Q49h5J+vE+6eg9UmZmpnbv3q0JEyZowoQJztf6acG2F154QX/729+0fPnyAv8A67qfeuihh7R3717NmDGjUP2FVatWKQiCQvc9Rx37p4L16tXLX/TDjYV3KShXrpx69+6tsWPHatWqVdq5c6cGDx6sHj166JlnnlGdOnVUsWJFTZw4US+//HJpDxenuMceeyy/dZf0498trV+/XjExMfrvf/+refPmadq0aZo1a5auvfZa/e1vf9O8efOUmJiYn3P07+eOFQTBcV/fqvRsmTFjhipVqqTevXtHlAeUhu+//15ZWVkF/gE1Li6u0EIlLy9PNWvW1KRJk5zPc7TgYFHmZc2aNbVkyRLNmjVLM2bM0IwZMzRx4kRdffXVeuGFF8J7szglHPtlw0/FxMQ4z/s/LW4Whs6dO6tRo0Z69dVXdfnll2vatGnav3+/hg4dmv+YvLw8ST/+nXft2rULPcexXTdc8xRwiY2NVadOndSpUyc1b95c11xzjV577TX9+c9/lnT8e6Sjx+aVV16pESNGOB97tM7NSy+9pJEjR+rCCy/Ub37zG9WsWVPly5fXQw89lF9v4af69++vmTNn6pFHHlGvXr1UqVKl/G15eXmKiYnRjBkznGP86X2eFPn92qmIhXcpOXz4sKQfv+l4/fXXValSJc2aNavAN4UTJ04skJOWlqa8vDytW7euwL8+rV69umQGjVPS1Vdfnf9TPKnwibVz587q3LmzHnjgAb388su64oorNHnyZI0aNSq0MfmKmL3zzjvq3bt3oXG6clJTU1W5cmWtWLGi0Lbly5erXLlyatCgQYH4qlWrCizq9+7dqy1btuQXcgMi8eKLL0r68ebHJz09Xe+99566du1apJub483L2NhYDRo0SIMGDVJeXp5uuukmPffcc/rjH//Ir6gQmqpVqzr/xOin304XVVpamiRpxYoVBf5UKDc3V+vWrdM555xT4PGXXXaZxo4dqz179uiVV15Ro0aN1Llz5/ztR3/qW7NmzUK5QHE5+o9SW7ZsKXJOamqqkpKSdOTIkeMem//973/VpEkTTZkypcB9z9FF/rE6d+6sX/3qVxo4cKAuvfRSTZ06Nf8fmdLT0xUEgRo3bqzmzZsXebyw8U91peDQoUN69913FRsbq1atWql8+fKKiYkp8C++69ev1xtvvFEg7+iN2TPPPFMgPm7cuNDHjFNXkyZNdM455+T/r2vXrpJ+/Jn4sd9cHK2W+dM/lwjD0eqxP20ZI/04t2bPnu38++6EhIRCjy9fvrz69eunN998s8BPxTMyMvTyyy+rW7duhX5eOGHChAI/3Ro/frwOHz6s888//8TeFE45H3zwge6//341btz4uK2ILrvsMh05ckT3339/oW2HDx/OP7aLMi937NhRYHu5cuXyvy0Je+7i1Jaenq7ly5crMzMzP/bVV1/ps88+i/i5zjnnHMXGxurvf/97gWP+n//8p7KysgpdB4YOHaqDBw/qhRde0MyZM3XZZZcV2N6/f38lJyfrwQcfdNZH+OmYgeOZM2eO89cdR+vBuP7EzVK+fHldcsklev3117V06dJC2396bB79Zvqnr/3FF1/o888/N5//nHPO0eTJkzVz5kxdddVV+d+wX3zxxSpfvrzuu+++Qu8lCIJC1xIcH994l4AZM2Zo+fLlkn78G4yXX35Zq1at0t13363k5GQNGDBAjz/+uM477zxdfvnl2rZtm55++mk1bdpUX3/9df7zdOjQQZdccomefPJJ7dixI7+d2MqVKyX5vwUEitsLL7ygZ555RhdddJHS09OVnZ2tf/zjH0pOTg7929/4+Hi1bt1ar7zyipo3b65q1aqpbdu2yszM1J49e5wL7w4dOui9997T448/rrp166px48Y666yz9Je//EWzZ89Wt27ddNNNN6lChQp67rnndPDgQT3yyCOFnic3N1d9+/bVZZddphUrVuiZZ55Rt27dNHjw4FDfM05uR68Dhw8fVkZGhj744APNnj1baWlpeuuttwr8vM+lZ8+eGj16tB566CEtWbJE/fr1U8WKFbVq1Sq99tprGjt2rIYMGVKkeTlq1Cjt3LlTffr0Uf369bVhwwaNGzdO7du3V6tWrUpid+AUde211+rxxx9X//79dd1112nbtm169tln1aZNm0KFLI8nNTVV99xzj+677z6dd955Gjx4cP45uVOnTgVatkrSGWecoaZNm+r3v/+9Dh48WOBn5tKPf8M9fvx4XXXVVTrjjDM0bNgwpaamauPGjXrnnXfUtWtXPfXUUye8D3BquOWWW5STk6OLLrpILVu2VG5urubOnZv/a4tIi5D99a9/1Zw5c3TWWWfp+uuvV+vWrbVz504tWrRI7733nnbu3ClJGjhwoKZMmaKLLrpIAwYM0Lp16/Tss8+qdevWBer1HOvCCy/M/5Oj5ORkPffcc0pPT9df/vIX3XPPPVq/fr0uvPBCJSUlad26dZo6dapuuOEG3XnnnSe0n045pVFK/VThaidWqVKloH379sH48eODvLy8/Mf+85//DJo1axbExcUFLVu2DCZOnOhsr7Rv375gzJgxQbVq1YLExMTgwgsvDFasWBFICv7617+W9FvEz0wk7cQWLVoUDB8+PGjYsGEQFxcX1KxZMxg4cGCwcOHC/Mf8tF3FsXRMqxirndiYMWOcrz937tygQ4cOQWxsbP5z3XnnnUHr1q2dj1++fHnQo0ePID4+PpBUoLXYokWLgv79+weJiYlB5cqVg969ewdz584tkH90Pn/00UfBDTfcEFStWjVITEwMrrjiimDHjh3H2104RR17HYiNjQ1q164dnHvuucHYsWODPXv2FHj8iBEjgoSEBPP5JkyYEHTo0CGIj48PkpKSgl/84hfBXXfdFfzwww9BEBRtXv73v/8N+vXrF9SsWTOIjY0NGjZsGIwePTrYsmVLODsBp4SitM4LgiB46aWXgiZNmgSxsbFB+/btg1mzZkXVTuyop556KmjZsmVQsWLFoFatWsGNN94Y7Nq1y/nav//97wNJQdOmTc3xzZkzJ+jfv3+QkpISVKpUKUhPTw9GjhxZYA4db54CM2bMCK699tqgZcuWQWJiYhAbGxs0bdo0uOWWW4KMjIz8x1n3OWlpaYVaoGZkZARjxowJGjRoEFSsWDGoXbt20Ldv32DChAn5j8nLywsefPDBIC0tLYiLiwtOP/304O233y40x6z7s2eeeSaQFNx55535sddffz3o1q1bkJCQECQkJAQtW7YMxowZE6xYsSL/MT179gzatGkT7e46ZcQEQRGqG6FMW7JkiU4//XS99NJLx/25IvBz1rp1aw0cOND5TfWJev7553XNNddowYIFZuEgAAAAwIWfmp9k9u/fX6iwzpNPPqly5cqpR48epTQqoPTl5uZq6NChhf5uDwAAAChtLLxPMo888oi+/PJL9e7dWxUqVMhvBXPDDTcUqr4MnEpiY2PNqp0AAABAaWLhfZLp0qWLZs+erfvvv1979+5Vw4YNde+99+r3v/99aQ8NAAAAAODA33gDAAAAABAi+ngDAAAAABAiFt4AAAAAAISIhTcAAAAAACEqcnG1mJiYMMcBlIoTKXFQnHOiXDn738Dy8vKKLccnNjbWGW/YsKGZ06ZNG2f8iy++MHO2bt0a2cCKWVpamrmtdevWzvjMmTPNnOIsk1Hcn2k0ysqcAMqSaOcFcwI/R1wngMKKMi/4xhsAAAAAgBCx8AYAAAAAIEQsvAEAAAAACBELbwAAAAAAQsTCGwAAAACAEMUERSxNSBVC/ByVlcqcvueytkVT5fq5554zt8XFxTnjBw8eNHNq1arljCclJZk51j63qqpL0uLFi53x+Ph4M+fQoUPOuFWJXZKys7Od8bVr15o5VapUccbfeustM+f11183t1msiufFXe28rMwJoCyhqjnw/3CdAAqjqjkAAAAAAKWMhTcAAAAAACFi4Q0AAAAAQIhYeAMAAAAAECIW3gAAAAAAhIiFNwAAAAAAIaKdWMis/Wa1BpLs9kDRtG+I5nM7kTYRkejSpYu5be7cuc54ixYtzJyVK1c64773U1ZaYkRzPPg89NBDznh6erqZ88MPPzjjvjZfR44cccZTUlLMnDp16jjjU6ZMMXOeffZZZ/zzzz83czIyMpzxffv2mTnbt293xsuXL2/mWJ9dtWrVzJx58+Y540888YSZY43B+gyiVVbmBFCW0E4M+H+4TgCF0U4MAAAAAIBSxsIbAAAAAIAQsfAGAAAAACBELLwBAAAAAAgRC28AAAAAAEJUobQHgMKKs6p4SVUo79Wrl7ntF7/4hTPerFkzM+fBBx90xn3VMPv16+eMHzx40MwpK6Kpat6kSRMzp23bts74xo0bzZy4uDhn3HcMWWPbvHlzxK+TlpZm5lx66aXOeE5OjpmTmZnpjGdnZ5s5VuVwX2V5q6q4VSVesj8fX/V063WiyQFQMqxrVkldm0uK79psvddocnznu2g6wkTz+ZRUDvBT0cwXn6SkJHNbt27dnPEZM2ZE/Dq+cVvz+fDhwxG/TjRKo/MT33gDAAAAABAiFt4AAAAAAISIhTcAAAAAACFi4Q0AAAAAQIhYeAMAAAAAECKqmgMAAIQgmgq4VtcDX6XfxMREZ3zhwoURv340onmf0eQUd6eGkho31ctxonzdb6x50bRpUzNn1KhR5rb9+/c74/v27TNzDhw44IzPnz/fzImmerlVidy3f6ycaF7f11mhKFh4H6O4y/VbOcV98bj66qud8Xnz5pk53bt3d8ZvvfVWM8dqkdSuXTszZ9WqVc74okWLzJzbb7/dGV+yZImZczKLZvL37dvX3Ga1VUlISDBzrJNmhQqRnyasm0BJ2rJlizNeo0YNM2fQoEHO+OLFi80cq1VGfHy8mWPtt0OHDpk51snedy6JjY11xq05KUkffvhhxK8DAACAsoGfmgMAAAAAECIW3gAAAAAAhIiFNwAAAAAAIWLhDQAAAABAiFh4AwAAAAAQIqqal0EtW7Z0xn3VpXv16uWMd+zY0cypWrWqM/7888+bOR9//LEz7qtQ3qFDB2e8U6dOZk5ubq4z7muNsHr1anPbz1Hr1q3NbVala19Vc2ufR1Pp36oOLkkVK1Z0xg8ePGjmWC0srOrgvuezXl+yuw1YFd8lKSUlxRmvVKmSmWPtN6uNkGRXNY+mIj6AklG5cmVn/LLLLjNzBg8e7Ix//fXXZo51zvV1Sti0aZMzXqVKFTPH6hbhu/5aHSu2b99u5lh8Y7PO+b7rkdUayNdlY/fu3RE9l29sPtZ1wncNs7bFxcWZOdZ7nThxomd0KGm+48u6d+nTp4+Zc84555jbvv/+e2fcdxxZ57pzzz3XzPm///s/ZzwjI8PMKc5uUb4OPNZ5IycnJ+LX+Sm+8QYAAAAAIEQsvAEAAAAACBELbwAAAAAAQsTCGwAAAACAELHwBgAAAAAgRFQ1BwAACMGgQYOc8fbt25s5f/jDH5xxX4Xy8847zxn3dWRYsmSJM964cWMz59ChQ854586dzRyrennt2rXNnOrVqzvj+/fvN3MyMzOd8RYtWpg5O3fujOi5JKldu3YRj82qhO6rdt6jRw9n3No3kv2Zfvfdd2aOVdm5WbNmZg5KntV5xsfXPahRo0bmNquCerly9ve1s2bNcsZPP/10M+eRRx5xxhcuXGjmfPPNN8647xg/88wznXHf/pk7d64z/vnnn5s5RcHC+xhWmfpoWeX1u3TpYuZs3brVGd+zZ4+Z889//tMZ//Wvf23m/PDDD874E088YebUrFnTGffttxUrVjjjVpsxyW4/4LuJONXaiaWnp5vbrBZTvjYk8fHxzrhvn1s3Yb62DlZ7Ml+rDOt1fO3ErDH42m9Z23wtNKyWE9b+lOx9kJqaauYAAADg5MVPzQEAAAAACBELbwAAAAAAQsTCGwAAAACAELHwBgAAAAAgRCy8AQAAAAAIEVXNj+GrrGxVL/ZV9LbaNPgqRbdt29YZ79Wrl5kzevRoZ9xqMSLZpf99tm3bFnGOVQndauMhSfXq1XPGr732WjPns88+c8aXLl3qGV3ZZ1Ui37t3r5mTlJTkjFvVwSV7n2/atMnMsY5jX8sJ3xyz+KqKW6yK59Y8jpY1tmrVqpk51j5t0qRJsYwJQNmwefNmZ9zXXaFjx47OuK/1TVZWVkRxSerZs6cz/tFHH5k5devWdcavuuoqM2fmzJnOuK+dkXWenjx5splj3WskJCSYOVZrLl9XilatWjnjvjZDO3bscMabN29u5lStWtUZ913Hre431r6RpG7dujnjEydONHMQHqvriW+tYXUCss4lkpSdnW1us+aM73i1ti1YsMDMsboRWWsnSTr77LOd8YsvvtjMseaMb2yjRo1yxn0tAIuCb7wBAAAAAAgRC28AAAAAAELEwhsAAAAAgBCx8AYAAAAAIEQsvAEAAAAACBFVzQEAiJBVedbHqkpb3N00KlSwL+2+atqR8nUvKO7uARar64Pvffr2XXFr2bKlM16/fn0zp2HDhs64r0NHenq6M+6rHN6uXTtnfM6cOWZOnTp1nPE1a9aYOTVq1HDG9+3bZ+Zs2LDB3GbJzc11xn2dOawK5b7Pp3LlypENTFJGRoYzPmjQoIhzfMdB06ZNnXFfdevk5GRn3FfZHUUTzXUiGvfff78zbs3X47GOcd951Zp/VtV8yT4ufdePRYsWOeNWhXTJHveYMWPMHKvLzJAhQ8ycomDhfQzfhx3NxXr//v3OuO+GpU+fPs74Sy+9ZOb86le/imxgJchq12Gd7CVp4cKFzrivjL/V1sl6/ZOFdeL0XfytY9XXosFqf7VixQozxzqOo2kn5pt7Vo5vTkZzwbPG4DvuzjjjDGfcd1NpLRaqVKliDw4AAAAnLX5qDgAAAABAiFh4AwAAAAAQIhbeAAAAAACEiIU3AAAAAAAhYuENAAAAAECIqGp+jOJuM5Kdne2Mf/zxx2aOb5vFavtw4MABMyea92pVivY9l1WVe+fOnWaOtd9mzJhh5tStW9cZT0tLM3NOBlbVbKsytmR/Tr72IFblcF/7CGsMvgrl0bRHiob1fL6xWfvtyJEjZo61f1JSUsycrVu3OuM7duwwc6y2QOvXrzdzEJ7iPF59FfijeZ3ibBkmSTfeeKMz/oc//MHMqVevXrGOwXLo0KESeZ1oWXM6NTXVzLHOD1bLMMnuJOF7HatdldVGR5J++ctfOuNffvmlmWO15vr666/NHKu7S+PGjc0cq81Wp06dzJy5c+c64z179jRzdu/e7Yxb12rJvob4OoBY53zfZ2pd460x+8bgu8dA0ZRU68Jdu3Y54752YlbXJcnuEuRrVWl1zfGtQ6zj1Xef1r17d2e8S5cuZo51jNesWdPMmTlzprntRPCNNwAAAAAAIWLhDQAAAABAiFh4AwAAAAAQIhbeAAAAAACEiIU3AAAAAAAhoqo5AADFJJoK5cVdhXz48OHmttNPP90Zv/TSS80cq/rt9u3bzZz//Oc/EY8tGrGxsc74XXfdZeb85S9/KdYx+FiVftetW2fmfPrpp874eeedZ+ZY1YGXL19u5uzZs8cZr127tpkzduxYZ7x3795mjlWFu2/fvmaOtQ+suGRX0p8+fbqZ065dO2e8VatWZs7kyZOdcV8VZKtCua+ye+fOnZ3xatWqmTmWb7/91txmHSNW1XuUPZUrV3bGfVXzfdtycnKc8aysLDPH6uBgHfuSfU30XUetcVv7QLK7Cviqpzdo0MDcdiJYeJcSq3WTZB8Ivkli8eX4WiQVJ+uiu3fvXjPHmnS+/Wbd4BT3TW1Jq1WrljPuOzEdPHjQGffdUFk3Yb6WIlYrH9/nZI3bd6xaJ2ffMWzl+NoPWWPz7QNrX/ta8qxcuTKi15ek9u3bO+O0EwMAACj7+Kk5AAAAAAAhYuENAAAAAECIWHgDAAAAABAiFt4AAAAAAISIhTcAAAAAACGiqnkpiaaiuC/Havfiqy5tiaYdjk9CQoIzPmLECDPn7bffdsZffvllM8eqkm61RThZpKenO+O+StsHDhxwxqtXr27mWJW2fe0WfGOwWNXLfceWdRz7jlWL7/1Yr+OrwG/l+Oae9V59ld1btGhhbkM4ojkXRnOObNq0qbnNavPVpUsXM6dfv37mtjVr1jjj33//vZljdTzwtYm54IILzG3FadiwYc74WWedVSKvfzxWV4qdO3eaOVYHg+TkZDPH6tbgy7HGdtppp5k577//vjPu6x5inbvuuOMOM8e6bl955ZVmTv369Z3xiRMnmjkfffSRM+5rj7ZixQpn3GrpJklDhgxxxqtUqWLmrFq1yhmPi4szc6yWar6xWa3GkpKSzBwUTTRdXKx7fatzjyTVrVvXGbe6rhxvm3WM5ebmmjnWnPUd41YLMl9rMKuFZHZ2tpmTkpLijPva+Vn7u2PHjmZOUfCNNwAAAAAAIWLhDQAAAABAiFh4AwAAAAAQIhbeAAAAAACEiIU3AAAAAAAhoqo5AKBMsiq/+irTWxVPfdVYLdFUKPdVcH3ggQec8aFDh5o5VqXYLVu2mDnz5883t1mdCHxVj5cvX+6MW1WkJen+++83t1lq1qzpjPv2z+OPP+6Mt2zZ0szp0KGDM/7ll196Rhcd6zkvvPBCM2f16tXOuO8z79mzpzOemppq5owdO9YZt6qdS9Jdd93ljPsqJP/mN79xxjMyMsyc2267zRn3deawKrufffbZZs5bb73ljI8bN87M6dWrlzNeu3ZtM+err75yxq0K6ZI0cOBAZ7xhw4ZmztKlS51xXwcSq4r9559/buagaKxriK/riVXV3HcetI69zMxMM8d3zreusVaXIklq0KCBM+679lrV0625LEkVKriXrr73Y503nn76aTPH6i5hvX5R/awX3sXdFutk5GtBFk2rsWjaoG3fvt0ZX7x4sZljlet/7rnnzByr7dbcuXM9oyv76tSp44xXqlTJzNm9e7cz7mvRYLUg851koplHvjYaFmsu+1rYRMO6ebQWc5K0a9cuZ9x3o2PtA99FzToOAAAAUPbxU3MAAAAAAELEwhsAAAAAgBCx8AYAAAAAIEQsvAEAAAAACBELbwAAAAAAQvSzrmp+qlQuj1Y0FcotVtl9yW6jMXnyZDPHaqPRv39/M8eqPL1p0yYz52RgtUHwVc22+KqA79+/P+Lns6pz+yqXF2dVc98+sNphWNXbJfsY8p1L9u7da26zWONOTk42c+rWrRvx65wMfN0nfNss0bQNs/Tt29fcdskllzjjl19+uZmzY8cOZ/zbb781c6w56ztWfC2XrHlutS2T7C4TW7duNXOs/WC1lvKN7ZtvvjFzrHY0vq4P2dnZ5rbitm/fPmf8/PPPN3OWLVvmjP/nP/8xc6zPvFq1amaOdW30HcPWcedrcfXFF18442vWrDFzXnzxRWf84osvNnOsa8uiRYvMnCZNmjjj1nElSVWrVnXGfW0Orc/H193F+uys15ekGTNmOOMjR440c6w2TNGcf1GQ1RUmmuuU1SpOsjuy+O6RomlpZrV8lOx7K+u6J9nj852/re4vVocZSfr++++dcd+57tFHH3XG582bZ+YUBd94AwAAAAAQIhbeAAAAAACEiIU3AAAAAAAhYuENAAAAAECIWHgDAAAAABCin3VVcwBA2earGF+cnRduvfVWc9uvfvUrZ7xWrVpmjlUl1VeB23o/vtex+Coo+/apVfnZ93yZmZnOuK+yumXu3Lnmtosuuiji5/vDH/7gjN90001mzsaNG53xK6+8MuLXP54WLVo4475K29Zx0rp1azPnk08+ccatqsqS1LVrV2f866+/NnP27NnjjLdq1crMsfb3FVdcYeZY++3tt982c6xqx926dTNzDh065IwvWbLEzLGq71vzRLK7BgwYMMDMWblypTP+5JNPmjnNmzd3xn3HgTX3GzRoYOaUNF+Fdas6t6+DivV81vEg+c+RFl8nmUhNnz7d3GZ1T/B1q7G6uEj2NcR3jFufg69CuW9/R5rj+3yssbVr187MycrKimxgRcTC+2cumnYBPr/97W+dcV/LkvHjxzvjV111lZljtR/wnXjS0tKc8eJsL1Qaomn1YZ3oatSoYeZYJ27fMRQN6+Tou0haY/C1Bov0uSR7bL6WHNYNle+4sz4f34UwmjZsAAAAKBu4kwMAAAAAIEQsvAEAAAAACBELbwAAAAAAQsTCGwAAAACAELHwBgAAAAAgRFQ1/5nzVS5v1KiRM37vvfeaOVZFaF+LgSFDhjjjq1atMnOs1hd169Y1c6JpS1BWxMXFRZzja9GQmprqjPtapOzevdsZ97U6OnjwoDMeTVsHX9Vu67P1tUix+NprWM/n+3wyMjKccatKvGRXl/dVqrfmsq/ielmaE2eccYYzfu6555o5Vksh37FvnSMSExPNHOvY37x5s5mTkpIS8disbb72X1bVfN/nHs1xFE27Id9csjoOnHnmmWbODz/84Iz7PjurrZvv+lK5cmVn/PrrrzdzomWNw+pWIUlbt251xlesWGHmWF1Cvv32WzPnu+++c8atFm2S9PnnnzvjtWvXNnMuuOACZ9y6TklSw4YNnXHfsWAdc5dffrmZ89Zbbznjvuue1WYrOzvbzKlTp05Ery/Z1x1f270vvvjCGf/yyy/NnF/+8pfOuNXOLEzW/YHvXrY4W3YVtx49ejjjl1xyiZljtfmzrgWS3QnI1ynFd8639rdvDNZn57t/Ks5roo+1H/bu3WvmXHzxxc74tGnTIn79n+IbbwAAAAAAQsTCGwAAAACAELHwBgAAAAAgRCy8AQAAAAAIEQtvAAAAAABCxMIbAAAAAIAQlbl2YtG0EjgZWe9TslvB+NoCWOX1W7ZsaeY8+uijzrivDYvVRuOOO+4wc3xtASzt27d3xps0aWLmWG1OTgZVq1aNOMfXfispKckZ97X5iqY1lzUvfZ+5dXz7WiBFwxqbb79Zc8xqmyZJCQkJzrivnVjz5s2dcV+7N2tsNWvWNHN87bDCcPPNN5vbrNYcvrZK1jGRm5tr5lhttnwtSKzX8bUusuaS73O32pZF08rL17bMN5es1i6+a5L1GfnGYH0Oe/bsMXOstkC7du2KOMd3XFnnxzBYn+0nn3xi5lifUe/evc2cDh06OONWizbJbr+1du1aM8dq8edjXQ8++OADM8f6jHwtyKzz9NKlS82c+fPnO+O+48f6fHxtk6z5tWnTJjOnWbNmzrivnZi1f6ZMmWLmWO2RfPs6LMV5r1+tWjVzm9V20trnvhzr2ibZ13rfPYV1j+K7hlWvXt0Zj2b+S9Hdb1jXZat9oyTNnTvXGfdde60Wbb7726ysLGfc13K1c+fO5rYTwTfeAAAAAACEiIU3AAAAAAAhYuENAAAAAECIWHgDAAAAABAiFt4AAAAAAISozFU1j6aiYTTVkKOptF2cfO/Tqn7pq2hYr149Z9xXbdyqKOqr5HfppZea24qT9fn4Ku/69k9ZV6VKFXObVSnVV53bqrS9YcMGM8eqUGxVDZbsz8NXXdL6bH3vJ5oc3xgifR1ftVqrOumyZcvMnIYNGzrjvmrd1r62PuvS8OKLL5rbFixY4Ix36dLFzGnbtq0znpaWZuZY1ZB9nQOsytO+87R17PkqAVvbfMeq9bn7ulz4qqT75oxl7969zrivgrt1LPvOJ9Z7iqb6rm9s1px95513zJy77rrL3OZjVUL2VXe35rRVFV+yK3f7Xueqq65yxmvVqmXm7Nixwxnfv3+/mWPNcd+x8MUXXzjjvq4rViXycePGmTlWNXirSrRkd5/wzf1GjRo543369DFzZsyY4Yx/+eWXZo51L+G7d7Iqqxd3p5GisO4/77//fjPH2u+++yrr3O7bT9b88x3H2dnZzrjvWm/td98cs6qDX3bZZWbOwoULzW3WddRXjd06xn1+8YtfRPT6kn28+tYA1rnBVz3dd59xIvjGGwAAAACAELHwBgAAAAAgRCy8AQAAAAAIEQtvAAAAAABCxMIbAAAAAIAQsfAGAAAAACBEZa6dWDRKuzWYj9UWwDfmaFqq3Xvvvc74Dz/8YOacdtppzvjQoUMjfv3iZu2DGjVqmDm+9gxlna810KFDh5xxXxspq/3VzJkzzRzreLBeX4quNZHV6shqZybZn62vbVI0rc6sMfj2gbWvfW1vrLZ8vtYW1j6oXLmymVPSfO1nrHZHVtsgH197t8aNGzvjTZs2NXOsNihWKyjJPlZ8+8CaL75jcvv27c641eJLsls+SXZLHF+rKmubr71NNO0drfNgNG2NrP0m2a3GwriXsNp5WS1AJalOnTrOuK/9j3WtT09PN3O2bNnijK9fv97MseaKr83Qhx9+6Iz7rnsrVqxwxqtVq2bm7Ny50xn3tUerWLGiM+6bQ1abIV9ORkaGM+5redW1a1dn3No3kjR9+nRnvEWLFmaO1TrNOj5OlK9l19///ndn3JoTkn2/6LuXLs7zk+91fOdIS0pKijPua2/117/+NeLXv/HGG81t1vnE19rx/fffd8bXrl1r5jRr1swZ97Xzs+6FrLks2dde371dZmamue1E8I03AAAAAAAhYuENAAAAAECIWHgDAAAAABAiFt4AAAAAAISIhTcAAAAAACEqc1XNo6kCblWF9FWytCokWtU3o1WcVVLvu+8+c9vhw4ed8Xbt2pk5F1100QmP6ShfdWmLNWbf8/mqmp/MfPvC4qv0az2fr/K7tc+tSrFSdFWardexKg1LdtVQX3Vr3zaLVQnZN48bNGjgjH/66admTlZWljPuq8ppVbG2KqCWBl9lbKsKv69abTTVrK3j1Xduj6aavcVXsdc6jnzdAayx+V7HVy06mq4CVrX91NRUMyc5OdkZ9x3j1v72XV+sqv7Z2dkRv86GDRvMnGhZ50Lf/j777LOdcasCsGQfQ76q2VOnTnXGfVXNu3Tp4oxbXQsk6ZtvvnHGfefo66+/3hn3XcOsquK+DiCzZs1yxn0V5H/72986423btjVzJkyY4Ix/9dVXZs4999zjjPu6LVjzrn79+maO1YEjrGvL1VdfbW6zKnevWbPGzLHOT74uIb7q+Bbr3OXbT5s2bXLGfR2HrHOaVRlfkl544QVn/MILLzRzpk2bZm6zuhf49mmHDh2c8d69e5s51nnLN8+t84bvumfxVaS3Pm/rnq+o+MYbAAAAAIAQsfAGAAAAACBELLwBAAAAAAgRC28AAAAAAELEwhsAAAAAgBCx8AYAAAAAIERlrp1YNO23Wrdu7Yz7Sr7v2bPHGbfK+EtSTk5OZAOLUr169Zxxq42HZLcm6d69e7GM6Xh8n5uvtVSkz9ewYcOIn+tk4DvurNY3Bw4cMHOsdgu+HKsVQ+3atc0cq3VTfHy8mVO9enVnfNu2bWZO1apVnXFfuyernZD1+pJ9fEXTJss3J6x9arXdkezPzrevyxKrXZyvjVw0rP3ha2NltRTxtU6x5pjvdSy+1mBWu5VoWhAe77Us1lzytcSxWsH5WoNZ+y6a1pO+HOta7ns/0bJaAO3fv9/M+e6775xxX/stq23Y9OnTzRyrxd7pp59u5sybN88Z97V7sq5vvvdjtTTztYm1zsW+17Fa4vlag1mt06x2ZpJ9zve1vVu7dq0z7pvDVjsx332Y1arSaq95onzXeqv9VlJSkplz8ODBiJ5Lss/tvpZU1r71tVy1WhT6ri3WucF3/2ad76yWgZL/fsNqJ+Zrw2a1APPdP1n3cL7zt3Us+669Vo6vZal1LDRv3tzMKQq+8QYAAAAAIEQsvAEAAAAACBELbwAAAAAAQsTCGwAAAACAELHwBgAAAAAgREWuam5VfoumCnlxv87cuXOLdQylbcKECc64r5LegAEDwhpOkVhVgSV/1cBIn69ly5YRP9fJwFdJ06rCnZKSYuZY+89XGdSaY76KsFblSV+1cavypFVdVrI/d6vCrmRXTrUqpEt2Belo9tvWrVvNnC1btjjjy5cvN3OaNWvmjPuOnVORVRHWV0XasmvXrhMdDk5xLVq0cMaHDRtm5ljV1X0VsDMzM53xyy+/3MxJT093xn3Vjhs3buyM169f38x59913nXFf9XTrumdV4PbxnfObNm3qjPsqlFsVz31js56vffv2Zk67du2ccasrj2RXdvfdo1nXlrPPPtvMORGbN282t1nX0++//97Msd5zjRo1zByr0ravkrs1x3ydGqLpfmF1KfLdh1j3Lr7306pVK3Ob1W3EVyneul767iGt8fnuIaO577S6nfi69mRlZTnjvjlbFHzjDQAAAABAiFh4AwAAAAAQIhbeAAAAAACEiIU3AAAAAAAhYuENAAAAAECIWHgDAAAAABCiIrcTK+62YcX5Ola7qunTp5s59erVc8YfeughM+c///lPZAPz+NOf/mRuO++885zxsWPHmjlLly494TGVJVZ7Bl9bkJNZYmJiVNssVquKs846y8yxWmU0aNDAzMnNzXXGrfYekt3WpHz58maO1XLC177F2m++1h87d+50xtu0aWPmWG1Jzj33XDPHaq/hO74PHjzojNeqVcvMAVC6rBZgVostyW4nZLWxkuzzwxdffBFxTuXKlc0cq6WR1eJHkjp06OCMW+dOyX8NsVjXg2XLlpk51vWgTp06Eb++71zcqFEjZ9x33du4caMzXq1aNTPH+kzXr19v5ljbfO0tT8SSJUvMbVOmTHHGr732WjPHar+3du1aM+fAgQPOuO9+y7qvslpVSXa7T9/nbn2GvpZw1topJyfHzLHamvqezzcGay5Z+1qy97d1bynZ5w3f+cRqNeY7b1mtEzMyMsycouAbbwAAAAAAQsTCGwAAAACAELHwBgAAAAAgRCy8AQAAAAAIEQtvAAAAAABCVOSq5r169XLGfZXn9uzZ44zv2rXLzNm3b58zblX5k+yKeb5Keunp6c74HXfcYea8//77zvi2bdvMnH79+jnjt956q5nz0UcfOeN33323mVOWRVOpvlw5978J+T7Tk1lqaqq5bfXq1c54SkqKmWNVnt26dauZY1XS9c09q5qnr1Kk1YXAen3Jrlbrq7BpVSD1jS0rK8sZ91U6tfaPVc1Uss9zLVu2NHOscZdUxwkAkUtOTnbGrS4Skl0duG/fvmbO4sWLnfH58+ebOVa3iG7dupk51n2drxK61a1h6tSpZo5VCb1hw4ZmTl5enjNuVb2W7Pfjex3rnG9dcyS74rLv2rJixQpn3LpOSXZXHOv+VbKvVVZV5zBZnYV8ldDvvPNOZ9yqJC/Zx76vMrZ13fZVKLf2ra+7ivV81r2TZN8H+I5J3zZr3L4c3/gizfFVDrfmjK/av3VuqF27tpnz9ddfO+MvvfSSmfPiiy+a247iG28AAAAAAELEwhsAAAAAgBCx8AYAAAAAIEQsvAEAAAAACBELbwAAAAAAQsTCGwAAAACAEBW5nZhVlt9Xrt9qkWS115CkQ4cOOeM7d+40c6wy8Zs2bTJzJk2a5Ixb5eMlu5VHly5dzJx27do545999pmZY7U087Vui4uLc8Z9raDKspycHGf83XffLeGRlAxf6ylrm+94sFpz+VpPWfvcOrak4m3vVqVKFXPbunXrIn4+q02F9T4lu42Hr2WgtU+tFmiSlJ2d7Yz72qNZc9nXHg1A6Vq2bJkz7mu/ZZ0H/vvf/5o51rmrdevWZs6WLVuccV/bSeseaeDAgWaO1TqtVq1aZo7V5uubb74xc6z7RF8LJKsl5ubNm80ca7/53o/1mfraV9WvX98Z912PvvvuO2e8Xr16Zo7VNuzVV181c06E1S5Wsu/nZ8yYYeZY23r37m3mWG3L0tLSzByrhavv/Vjz0tdOzHcfYLGOCd89n+8Yt+43fPc1vrZqFmt81lpQsu/hfJ/D7NmznXFrvkjS3LlzzW0ngm+8AQAAAAAIEQtvAAAAAABCxMIbAAAAAIAQsfAGAAAAACBELLwBAAAAAAhRTOAreffTBxoVgotb9erVnXGruqMkVatWLeIc6/34Khq2atXKGU9KSjJzPv30U2f85ZdfNnN81dhPFVa1/EWLFpk51nHgU8TD36k458TQoUPNbXfeeaczvn79ejOnadOmzvjq1avNnMTERGd83759Zo5VUdtXlTMhIcEZj6bCra/6bjSsCrdVq1Y1c6xjaMOGDWbOmWee6Yxb71OStm/f7ow/+eSTZs5HH31kbrOUlTkBlCXRzgvmBH6OTsXrRMuWLZ3xGjVqmDlW1Xrf+sS6t/NV+l6zZo25DSWnKPOCb7wBAAAAAAgRC28AAAAAAELEwhsAAAAAgBCx8AYAAAAAIEQsvAEAAAAACBELbwAAAAAAQlShtAdwrB07dkQUx8+T1U7h6aefLtmBlJBly5aZ23Jycpzxdu3amTm///3vnXGr/Zdkt/Kz2lhJdvutZs2amTmDBw92xn3t0fLy8pzx5s2bmzk7d+50xitWrGjmvPvuu854uXL2v1GmpKQ44779ZuV06NDBzLHaknz22WdmDgAAOHHLly8vtudaunRpsT0XTi584w0AAAAAQIhYeAMAAAAAECIW3gAAAAAAhIiFNwAAAAAAIWLhDQAAAABAiGKCIAiK9MCYmLDHApS4Ih7+TiU1J84//3xnvFu3bmbOfffd54zn5uYWy5hwYqyq5mPHjjVzPv30U2f8//7v/4plTEedDHMCKGnRzgvmBH6OuE4AhRVlXvCNNwAAAAAAIWLhDQAAAABAiFh4AwAAAAAQIhbeAAAAAACEiIU3AAAAAAAhYuENAAAAAECIitxODAAAAAAARI5vvAEAAAAACBELbwAAAAAAQsTCGwAAAACAELHwBgAAAAAgRCy8AQAAAAAIEQtvAAAAAABCxMIbAAAAAIAQsfAGAAAAACBELLwBAAAAAAjR/wfedA4qFmgyFQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess (flatten and normalize) images\n",
        "def preprocess_image_data(images):\n",
        "    # Flatten each 28x28 image into a 1D array and normalize pixel values to [0, 1]\n",
        "    return np.array([img.flatten() / 255.0 for img in images])\n",
        "\n",
        "# Loading the Fashion MNIST dataset\n",
        "(train_data, train_labels), (test_data, y_test_data) = fashion_mnist.load_data() # Load test_data here\n",
        "\n",
        "# Preprocess the training and test datasets\n",
        "x_total_train_processed = preprocess_image_data(train_data)\n",
        "x_test_processed = preprocess_image_data(test_data)\n",
        "\n",
        "flattened_test_data = []\n",
        "\n",
        "\n",
        "for i in range(0, len(test_data)):\n",
        "    flattened_test_data.append(test_data[i].flatten()/255.0)\n",
        "\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "train_validation_split_ratio = 0.9  # 90% training, 10% validation\n",
        "split_index = int(len(x_total_train_processed) * train_validation_split_ratio)\n",
        "\n",
        "x_train_final = x_total_train_processed[:split_index]\n",
        "y_train_final = train_labels[:split_index] # Use train_labels instead of y_total_train_data\n",
        "x_validation_final = x_total_train_processed[split_index:]\n",
        "y_validation_final = train_labels[split_index:] # Use train_labels instead of y_total_train_data"
      ],
      "metadata": {
        "id": "C7EpzZkYAl1L"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_neurons, output_neurons, config):\n",
        "\n",
        "        self.hidden_layers = config[\"hidden_layers\"]\n",
        "        self.hidden_neurons = config[\"hl_size\"]\n",
        "        self.input_neurons = input_neurons # This is where input_size should be used\n",
        "        self.output_neurons = output_neurons #This is where output_size should be used\n",
        "        self.total_layers = self.hidden_layers + 1\n",
        "        self.output_layer_index = self.total_layers - 1\n",
        "        self.config = config\n",
        "        self.input_layer_neurons = input_neurons # Store the input layer size for later use\n",
        "        self.output_layer_neurons = output_neurons # Store output layer size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        if self.config[\"initialization\"] == \"random\":\n",
        "            self._initialize_random()\n",
        "        elif self.config[\"initialization\"] == \"xavier\":\n",
        "            self._initialize_xavier()\n",
        "\n",
        "    def _initialize_random(self):\n",
        "        \"\"\"Initializes weights and biases with small random values.\"\"\"\n",
        "        for i in range(self.total_layers):\n",
        "            if i == 0:\n",
        "                layer_weights = np.random.randn(self.hidden_neurons, self.input_neurons) * 0.01\n",
        "                layer_biases = np.random.randn(self.hidden_neurons, 1) * 0.01\n",
        "            elif i == self.output_layer_index:\n",
        "                layer_weights = np.random.randn(self.output_neurons, self.hidden_neurons) * 0.01\n",
        "                layer_biases = np.random.randn(self.output_neurons, 1) * 0.01\n",
        "            else:\n",
        "                layer_weights = np.random.randn(self.hidden_neurons, self.hidden_neurons) * 0.01\n",
        "                layer_biases = np.random.randn(self.hidden_neurons, 1) * 0.01\n",
        "            self.weights.append(layer_weights)\n",
        "            self.biases.append(layer_biases)\n",
        "\n",
        "    def _initialize_xavier(self):\n",
        "        \"\"\"Initializes weights using Xavier initialization and biases with zeros.\"\"\"\n",
        "        for i in range(self.total_layers):\n",
        "            if i == 0:\n",
        "                scale = np.sqrt(2.0 / (self.hidden_neurons + self.input_neurons))\n",
        "                layer_weights = np.random.randn(self.hidden_neurons, self.input_neurons) * scale\n",
        "                layer_biases = np.zeros((self.hidden_neurons, 1))\n",
        "            elif i == self.output_layer_index:\n",
        "                scale = np.sqrt(2.0 / (self.hidden_neurons + self.output_neurons))\n",
        "                layer_weights = np.random.randn(self.output_neurons, self.hidden_neurons) * scale\n",
        "                layer_biases = np.zeros((self.output_neurons, 1))\n",
        "            else:\n",
        "                scale = np.sqrt(2.0 / (self.hidden_neurons + self.hidden_neurons))\n",
        "                layer_weights = np.random.randn(self.hidden_neurons, self.hidden_neurons) * scale\n",
        "                layer_biases = np.zeros((self.hidden_neurons, 1))\n",
        "            self.weights.append(layer_weights)\n",
        "            self.biases.append(layer_biases)\n",
        "\n",
        "    # Method to perform backpropagation and compute gradients for weights and biases\n",
        "    def compute_gradients(self, activations, pre_activations, true_label, input_data):\n",
        "        # Initialize lists to store gradients for activations, pre-activations, weights, and biases\n",
        "        grad_activations = [None] * self.total_layers\n",
        "        grad_pre_activations = [None] * self.total_layers\n",
        "        grad_weights = [None] * self.total_layers\n",
        "        grad_biases = [None] * self.total_layers\n",
        "\n",
        "        # Create one-hot encoded vector for the true label\n",
        "        one_hot_label = np.zeros((self.output_layer_neurons, 1))\n",
        "        one_hot_label[true_label] = 1\n",
        "\n",
        "        # Compute the gradient of the loss with respect to the output layer's pre-activations\n",
        "        if self.config[\"loss\"] == \"cross_entropy\":\n",
        "            grad_pre_activations[self.total_layers - 1] = -(one_hot_label - activations[self.total_layers - 1])\n",
        "        elif self.config[\"loss\"] == \"mean_squared_error\":\n",
        "            output_activation = activations[self.total_layers - 1]\n",
        "            grad_pre_activations[self.total_layers - 1] = (output_activation - one_hot_label) * output_activation * (1 - output_activation)\n",
        "\n",
        "        # Iterate through layers in reverse order to compute gradients\n",
        "        for layer_idx in range(self.total_layers - 1, -1, -1):\n",
        "            if layer_idx == 0:\n",
        "                # Compute weight gradients for the input layer\n",
        "                grad_weights[layer_idx] = np.matmul(grad_pre_activations[layer_idx], input_data.reshape(1, -1))\n",
        "            else:\n",
        "                # Compute weight gradients for hidden and output layers\n",
        "                grad_weights[layer_idx] = np.matmul(grad_pre_activations[layer_idx], activations[layer_idx - 1].T)\n",
        "\n",
        "            # Bias gradients are the same as the pre-activation gradients\n",
        "            grad_biases[layer_idx] = np.copy(grad_pre_activations[layer_idx])\n",
        "\n",
        "            if layer_idx - 1 >= 0:\n",
        "                # Compute gradients for the previous layer's activations\n",
        "                grad_activations[layer_idx - 1] = np.matmul(self.weights[layer_idx].T, grad_pre_activations[layer_idx])\n",
        "                # Compute gradients for the previous layer's pre-activations\n",
        "                grad_pre_activations[layer_idx - 1] = grad_activations[layer_idx - 1] * self.compute_activation_derivative(self.config[\"activation\"], pre_activations[layer_idx - 1])\n",
        "\n",
        "        # Return the computed gradients for weights and biases\n",
        "        return grad_weights, grad_biases\n",
        "\n",
        "    # Takes a flattened image as input and returns activations and pre-activations for all layers\n",
        "    def forward_propagate(self, input_data):\n",
        "        # Initialize lists to store pre-activations and activations for each layer\n",
        "        pre_activations = [None] * self.total_layers\n",
        "        activations = [None] * self.total_layers\n",
        "\n",
        "        # Iterate through each layer in the network\n",
        "        for layer_idx in range(self.total_layers):\n",
        "            if layer_idx == 0:\n",
        "                # Compute pre-activation for the input layer\n",
        "                pre_activations[layer_idx] = np.matmul(self.weights[layer_idx], input_data.reshape(self.input_layer_neurons, 1)) + self.biases[layer_idx]\n",
        "                # Apply activation function to get the activation for the input layer\n",
        "                activations[layer_idx] = self.apply_activation(self.config[\"activation\"], pre_activations[layer_idx])\n",
        "\n",
        "            elif layer_idx == self.total_layers - 1:\n",
        "                # Compute pre-activation for the output layer\n",
        "                pre_activations[layer_idx] = np.matmul(self.weights[layer_idx], activations[layer_idx - 1]) + self.biases[layer_idx]\n",
        "                # Apply softmax activation for the output layer\n",
        "                activations[layer_idx] = self.softmax(pre_activations[layer_idx])\n",
        "\n",
        "            else:\n",
        "                # Compute pre-activation for hidden layers\n",
        "                pre_activations[layer_idx] = np.matmul(self.weights[layer_idx], activations[layer_idx - 1]) + self.biases[layer_idx]\n",
        "                # Apply activation function for hidden layers\n",
        "                activations[layer_idx] = self.apply_activation(self.config[\"activation\"], pre_activations[layer_idx])\n",
        "\n",
        "        return activations, pre_activations\n",
        "\n",
        "    # Method to perform gradient descent with momentum optimization\n",
        "    def momentum_based_gradient_descent(self, training_data, training_labels, validation_data, validation_labels):\n",
        "        # Initialize previous updates for weights and biases with zeros\n",
        "        previous_weight_updates = [np.zeros_like(weight) for weight in self.weights]\n",
        "        previous_bias_updates = [np.zeros_like(bias) for bias in self.biases]\n",
        "\n",
        "        # Temporary variables to store current updates\n",
        "        current_weight_updates = [np.zeros_like(weight) for weight in self.weights]\n",
        "        current_bias_updates = [np.zeros_like(bias) for bias in self.biases]\n",
        "\n",
        "        # Hyperparameters\n",
        "        momentum_beta = self.config[\"momentum_beta\"]\n",
        "        batch_size = self.config[\"batch_size\"]\n",
        "        learning_rate = self.config[\"learning_rate\"]\n",
        "        weight_decay = self.config[\"weight_decay\"]\n",
        "\n",
        "        # Training loop over epochs\n",
        "        for epoch in range(self.config[\"epochs\"]):\n",
        "            # Process data in mini-batches\n",
        "            for batch_start in range(0, len(training_data), batch_size):\n",
        "                batch_data = training_data[batch_start:batch_start + batch_size]\n",
        "                batch_labels = training_labels[batch_start:batch_start + batch_size]\n",
        "\n",
        "                # Initialize gradients for weights and biases\n",
        "                weight_gradients = [np.zeros_like(weight) for weight in self.weights]\n",
        "                bias_gradients = [np.zeros_like(bias) for bias in self.biases]\n",
        "\n",
        "                # Compute gradients for each sample in the batch\n",
        "                for sample_index in range(len(batch_data)):\n",
        "                    activations, pre_activations = self.forward_propagate(batch_data[sample_index])\n",
        "                    sample_weight_gradients, sample_bias_gradients = self.compute_gradients(activations, pre_activations, batch_labels[sample_index], batch_data[sample_index])\n",
        "\n",
        "                    # Accumulate gradients\n",
        "                    for layer_index in range(self.total_layers):\n",
        "                        weight_gradients[layer_index] += sample_weight_gradients[layer_index]\n",
        "                        bias_gradients[layer_index] += sample_bias_gradients[layer_index]\n",
        "\n",
        "                # Update weights and biases using momentum\n",
        "                for layer_index in range(self.total_layers):\n",
        "                    # Update weights\n",
        "                    current_weight_updates[layer_index] = momentum_beta * previous_weight_updates[layer_index] + learning_rate * weight_gradients[layer_index]\n",
        "                    self.weights[layer_index] -= current_weight_updates[layer_index] + weight_decay * self.weights[layer_index]\n",
        "                    previous_weight_updates[layer_index] = current_weight_updates[layer_index]\n",
        "\n",
        "                    # Update biases\n",
        "                    current_bias_updates[layer_index] = momentum_beta * previous_bias_updates[layer_index] + learning_rate * bias_gradients[layer_index]\n",
        "                    self.biases[layer_index] -= current_bias_updates[layer_index]\n",
        "                    previous_bias_updates[layer_index] = current_bias_updates[layer_index]\n",
        "\n",
        "            # Calculate and log loss at specific intervals\n",
        "            if (self.config[\"epochs\"] == 10 and epoch % 2 == 1) or self.config[\"epochs\"] == 5:\n",
        "                self.calculate_loss(training_data, training_labels, validation_data, validation_labels, epoch)\n",
        "\n",
        "    # Method to perform stochastic gradient descent with mini-batches\n",
        "    def stochastic_gradient_descent(self, training_data, training_labels, validation_data, validation_labels):\n",
        "        # Hyperparameters\n",
        "        batch_size = self.config[\"batch_size\"]\n",
        "        learning_rate = self.config[\"learning_rate\"]\n",
        "        weight_decay = self.config[\"weight_decay\"]\n",
        "\n",
        "        # Training loop over epochs\n",
        "        for epoch in range(self.config[\"epochs\"]):\n",
        "            # Process data in mini-batches\n",
        "            for batch_start in range(0, len(training_data), batch_size):\n",
        "                batch_data = training_data[batch_start:batch_start + batch_size]\n",
        "                batch_labels = training_labels[batch_start:batch_start + batch_size]\n",
        "\n",
        "                # Initialize gradients for weights and biases\n",
        "                weight_gradients = [np.zeros_like(weight) for weight in self.weights]\n",
        "                bias_gradients = [np.zeros_like(bias) for bias in self.biases]\n",
        "\n",
        "                # Compute gradients for each sample in the batch\n",
        "                for sample_index in range(len(batch_data)):\n",
        "                    # Forward pass\n",
        "                    activations, pre_activations = self.forward_propagate(batch_data[sample_index])\n",
        "                    # Backward pass\n",
        "                    sample_weight_gradients, sample_bias_gradients = self.compute_gradients(activations, pre_activations, batch_labels[sample_index], batch_data[sample_index])\n",
        "\n",
        "                    # Accumulate gradients\n",
        "                    for layer_index in range(self.total_layers):\n",
        "                        weight_gradients[layer_index] += sample_weight_gradients[layer_index]\n",
        "                        bias_gradients[layer_index] += sample_bias_gradients[layer_index]\n",
        "\n",
        "                # Update weights and biases using the computed gradients\n",
        "                for layer_index in range(self.total_layers):\n",
        "                    # Update weights with weight decay\n",
        "                    self.weights[layer_index] -= learning_rate * weight_gradients[layer_index] + weight_decay * self.weights[layer_index]\n",
        "                    # Update biases\n",
        "                    self.biases[layer_index] -= learning_rate * bias_gradients[layer_index]\n",
        "\n",
        "            # Calculate and log loss at specific intervals\n",
        "            if (self.config[\"epochs\"] == 10 and epoch % 2 == 1) or self.config[\"epochs\"] == 5:\n",
        "                self.calculate_loss(training_data, training_labels, validation_data, validation_labels, epoch)\n",
        "\n",
        "    # Method to perform Nesterov Accelerated Gradient Descent\n",
        "    def nesterov_gradient_descent(self, training_data, training_labels, validation_data, validation_labels):\n",
        "        # Initialize previous updates for weights and biases with zeros\n",
        "        previous_weight_updates = [np.zeros_like(weight) for weight in self.weights]\n",
        "        previous_bias_updates = [np.zeros_like(bias) for bias in self.biases]\n",
        "\n",
        "        # Temporary variables to store current updates\n",
        "        current_weight_updates = [np.zeros_like(weight) for weight in self.weights]\n",
        "        current_bias_updates = [np.zeros_like(bias) for bias in self.biases]\n",
        "\n",
        "        # Hyperparameters\n",
        "        momentum_beta = self.config[\"momentum_beta\"]\n",
        "        batch_size = self.config[\"batch_size\"]\n",
        "        learning_rate = self.config[\"learning_rate\"]\n",
        "        weight_decay = self.config[\"weight_decay\"]\n",
        "\n",
        "        # Training loop over epochs\n",
        "        for epoch in range(self.config[\"epochs\"]):\n",
        "            # Process data in mini-batches\n",
        "            for batch_start in range(0, len(training_data), batch_size):\n",
        "                batch_data = training_data[batch_start:batch_start + batch_size]\n",
        "                batch_labels = training_labels[batch_start:batch_start + batch_size]\n",
        "\n",
        "                # Initialize gradients for weights and biases\n",
        "                weight_gradients = [np.zeros_like(weight) for weight in self.weights]\n",
        "                bias_gradients = [np.zeros_like(bias) for bias in self.biases]\n",
        "\n",
        "                # Compute lookahead updates for weights and biases\n",
        "                for layer_index in range(self.total_layers):\n",
        "                    current_weight_updates[layer_index] = momentum_beta * previous_weight_updates[layer_index]\n",
        "                    current_bias_updates[layer_index] = momentum_beta * previous_bias_updates[layer_index]\n",
        "\n",
        "                # Update parameters temporarily for lookahead gradient calculation\n",
        "                self.update_parameters_with_momentum(current_weight_updates, current_bias_updates)\n",
        "\n",
        "                # Compute gradients for each sample in the batch\n",
        "                for sample_index in range(len(batch_data)):\n",
        "                    # Forward pass\n",
        "                    activations, pre_activations = self.forward_propagate(batch_data[sample_index])\n",
        "                    # Backward pass\n",
        "                    sample_weight_gradients, sample_bias_gradients = self.compute_gradients(activations, pre_activations, batch_labels[sample_index], batch_data[sample_index])\n",
        "\n",
        "                    # Accumulate gradients\n",
        "                    for layer_index in range(self.total_layers):\n",
        "                        weight_gradients[layer_index] += sample_weight_gradients[layer_index]\n",
        "                        bias_gradients[layer_index] += sample_bias_gradients[layer_index]\n",
        "\n",
        "                # Update weights and biases using Nesterov Accelerated Gradient Descent\n",
        "                for layer_index in range(self.total_layers):\n",
        "                    # Update weights\n",
        "                    previous_weight_updates[layer_index] = current_weight_updates[layer_index] + learning_rate * weight_gradients[layer_index]\n",
        "                    self.weights[layer_index] -= previous_weight_updates[layer_index] + weight_decay * self.weights[layer_index]\n",
        "\n",
        "                    # Update biases\n",
        "                    previous_bias_updates[layer_index] = current_bias_updates[layer_index] + learning_rate * bias_gradients[layer_index]\n",
        "                    self.biases[layer_index] -= previous_bias_updates[layer_index]\n",
        "\n",
        "            # Calculate and log loss at specific intervals\n",
        "            if (self.config[\"epochs\"] == 10 and epoch % 2 == 1) or self.config[\"epochs\"] == 5:\n",
        "                self.calculate_loss(training_data, training_labels, validation_data, validation_labels, epoch)\n",
        "\n",
        "    # Method to perform RMSProp optimization\n",
        "    def rmsprop_optimization(self, training_data, training_labels, validation_data, validation_labels):\n",
        "        # Initialize exponentially weighted averages of squared gradients for weights and biases\n",
        "        squared_grad_weights = [np.zeros_like(weight) for weight in self.weights]\n",
        "        squared_grad_biases = [np.zeros_like(bias) for bias in self.biases]\n",
        "\n",
        "        # Hyperparameters\n",
        "        decay_rate = self.config[\"rms_beta\"]  # Decay rate for the moving average\n",
        "        epsilon = 1e-4  # Small constant to avoid division by zero\n",
        "        batch_size = self.config[\"batch_size\"]\n",
        "        learning_rate = self.config[\"learning_rate\"]\n",
        "        weight_decay = self.config[\"weight_decay\"]\n",
        "\n",
        "        # Training loop over epochs\n",
        "        for epoch in range(self.config[\"epochs\"]):\n",
        "            # Process data in mini-batches\n",
        "            for batch_start in range(0, len(training_data), batch_size):\n",
        "                batch_data = training_data[batch_start:batch_start + batch_size]\n",
        "                batch_labels = training_labels[batch_start:batch_start + batch_size]\n",
        "\n",
        "                # Initialize gradients for weights and biases\n",
        "                weight_gradients = [np.zeros_like(weight) for weight in self.weights]\n",
        "                bias_gradients = [np.zeros_like(bias) for bias in self.biases]\n",
        "\n",
        "                # Compute gradients for each sample in the batch\n",
        "                for sample_index in range(len(batch_data)):\n",
        "                    # Forward pass\n",
        "                    activations, pre_activations = self.forward_propagate(batch_data[sample_index])\n",
        "                    # Backward pass\n",
        "                    sample_weight_gradients, sample_bias_gradients = self.compute_gradients(activations, pre_activations, batch_labels[sample_index], batch_data[sample_index])\n",
        "\n",
        "                    # Accumulate gradients\n",
        "                    for layer_index in range(self.total_layers):\n",
        "                        weight_gradients[layer_index] += sample_weight_gradients[layer_index]\n",
        "                        bias_gradients[layer_index] += sample_bias_gradients[layer_index]\n",
        "\n",
        "                # Update exponentially weighted averages of squared gradients and parameters\n",
        "                for layer_index in range(self.total_layers):\n",
        "                    # Update squared gradients for weights\n",
        "                    squared_grad_weights[layer_index] = decay_rate * squared_grad_weights[layer_index] + (1 - decay_rate) * (weight_gradients[layer_index] ** 2)\n",
        "                    # Update weights with RMSProp and weight decay\n",
        "                    self.weights[layer_index] -= learning_rate * weight_gradients[layer_index] / (np.sqrt(squared_grad_weights[layer_index]) + epsilon) + weight_decay * self.weights[layer_index]\n",
        "\n",
        "                    # Update squared gradients for biases\n",
        "                    squared_grad_biases[layer_index] = decay_rate * squared_grad_biases[layer_index] + (1 - decay_rate) * (bias_gradients[layer_index] ** 2)\n",
        "                    # Update biases with RMSProp\n",
        "                    self.biases[layer_index] -= learning_rate * bias_gradients[layer_index] / (np.sqrt(squared_grad_biases[layer_index]) + epsilon)\n",
        "\n",
        "            # Calculate and log loss at specific intervals\n",
        "            if (self.config[\"epochs\"] == 10 and epoch % 2 == 1) or self.config[\"epochs\"] == 5:\n",
        "                self.calculate_loss(training_data, training_labels, validation_data, validation_labels, epoch)\n",
        "\n",
        "    # Method to perform Adam optimization\n",
        "    def adam_optimization(self, training_data, training_labels, validation_data, validation_labels):\n",
        "        # Initialize first and second moment estimates for weights and biases\n",
        "        first_moment_weights = [np.zeros_like(weight) for weight in self.weights]\n",
        "        first_moment_biases = [np.zeros_like(bias) for bias in self.biases]\n",
        "        second_moment_weights = [np.zeros_like(weight) for weight in self.weights]\n",
        "        second_moment_biases = [np.zeros_like(bias) for bias in self.biases]\n",
        "\n",
        "        # Initialize bias-corrected moment estimates\n",
        "        first_moment_weights_hat = [np.zeros_like(weight) for weight in self.weights]\n",
        "        first_moment_biases_hat = [np.zeros_like(bias) for bias in self.biases]\n",
        "        second_moment_weights_hat = [np.zeros_like(weight) for weight in self.weights]\n",
        "        second_moment_biases_hat = [np.zeros_like(bias) for bias in self.biases]\n",
        "\n",
        "        # Hyperparameters\n",
        "        beta1 = self.config[\"beta1\"]  # Exponential decay rate for the first moment estimates\n",
        "        beta2 = self.config[\"beta2\"]  # Exponential decay rate for the second moment estimates\n",
        "        epsilon = self.config[\"eps\"]  # Small constant to avoid division by zero\n",
        "        batch_size = self.config[\"batch_size\"]\n",
        "        learning_rate = self.config[\"learning_rate\"]\n",
        "        weight_decay = self.config[\"weight_decay\"]\n",
        "\n",
        "        # Training loop over epochs\n",
        "        for epoch in range(self.config[\"epochs\"]):\n",
        "            # Process data in mini-batches\n",
        "            for batch_start in range(0, len(training_data), batch_size):\n",
        "                batch_data = training_data[batch_start:batch_start + batch_size]\n",
        "                batch_labels = training_labels[batch_start:batch_start + batch_size]\n",
        "\n",
        "                # Initialize gradients for weights and biases\n",
        "                weight_gradients = [np.zeros_like(weight) for weight in self.weights]\n",
        "                bias_gradients = [np.zeros_like(bias) for bias in self.biases]\n",
        "\n",
        "                # Compute gradients for each sample in the batch\n",
        "                for sample_index in range(len(batch_data)):\n",
        "                    # Forward pass\n",
        "                    activations, pre_activations = self.forward_propagate(batch_data[sample_index])\n",
        "                    # Backward pass\n",
        "                    sample_weight_gradients, sample_bias_gradients = self.compute_gradients(activations, pre_activations, batch_labels[sample_index], batch_data[sample_index])\n",
        "\n",
        "                    # Accumulate gradients\n",
        "                    for layer_index in range(self.total_layers):\n",
        "                        weight_gradients[layer_index] += sample_weight_gradients[layer_index]\n",
        "                        bias_gradients[layer_index] += sample_bias_gradients[layer_index]\n",
        "\n",
        "                # Update first and second moment estimates and parameters\n",
        "                for layer_index in range(self.total_layers):\n",
        "                    # Update first moment estimates (mean)\n",
        "                    first_moment_weights[layer_index] = beta1 * first_moment_weights[layer_index] + (1 - beta1) * weight_gradients[layer_index]\n",
        "                    first_moment_biases[layer_index] = beta1 * first_moment_biases[layer_index] + (1 - beta1) * bias_gradients[layer_index]\n",
        "\n",
        "                    # Update second moment estimates (uncentered variance)\n",
        "                    second_moment_weights[layer_index] = beta2 * second_moment_weights[layer_index] + (1 - beta2) * (weight_gradients[layer_index] ** 2)\n",
        "                    second_moment_biases[layer_index] = beta2 * second_moment_biases[layer_index] + (1 - beta2) * (bias_gradients[layer_index] ** 2)\n",
        "\n",
        "                    # Compute bias-corrected moment estimates\n",
        "                    first_moment_weights_hat[layer_index] = first_moment_weights[layer_index] / (1 - np.power(beta1, epoch + 1))\n",
        "                    first_moment_biases_hat[layer_index] = first_moment_biases[layer_index] / (1 - np.power(beta1, epoch + 1))\n",
        "                    second_moment_weights_hat[layer_index] = second_moment_weights[layer_index] / (1 - np.power(beta2, epoch + 1))\n",
        "                    second_moment_biases_hat[layer_index] = second_moment_biases[layer_index] / (1 - np.power(beta2, epoch + 1))\n",
        "\n",
        "                    # Update weights and biases using Adam optimization\n",
        "                    self.weights[layer_index] -= learning_rate * first_moment_weights_hat[layer_index] / (np.sqrt(second_moment_weights_hat[layer_index]) + epsilon) + weight_decay * self.weights[layer_index]\n",
        "                    self.biases[layer_index] -= learning_rate * first_moment_biases_hat[layer_index] / (np.sqrt(second_moment_biases_hat[layer_index]) + epsilon)\n",
        "\n",
        "            # Calculate and log loss at specific intervals\n",
        "            if (self.config[\"epochs\"] == 10 and epoch % 2 == 1) or self.config[\"epochs\"] == 5:\n",
        "                self.calculate_loss(training_data, training_labels, validation_data, validation_labels, epoch)\n",
        "    # Method to perform NAdam optimization\n",
        "    def nadam_optimization(self, training_data, training_labels, validation_data, validation_labels):\n",
        "        # Initialize first and second moment estimates for weights and biases\n",
        "        first_moment_weights = [np.zeros_like(weight) for weight in self.weights]\n",
        "        first_moment_biases = [np.zeros_like(bias) for bias in self.biases]\n",
        "        second_moment_weights = [np.zeros_like(weight) for weight in self.weights]\n",
        "        second_moment_biases = [np.zeros_like(bias) for bias in self.biases]\n",
        "\n",
        "        # Initialize bias-corrected moment estimates\n",
        "        first_moment_weights_hat = [np.zeros_like(weight) for weight in self.weights]\n",
        "        first_moment_biases_hat = [np.zeros_like(bias) for bias in self.biases]\n",
        "        second_moment_weights_hat = [np.zeros_like(weight) for weight in self.weights]\n",
        "        second_moment_biases_hat = [np.zeros_like(bias) for bias in self.biases]\n",
        "\n",
        "        # Hyperparameters\n",
        "        beta1 = self.config[\"beta1\"]  # Exponential decay rate for the first moment estimates\n",
        "        beta2 = self.config[\"beta2\"]  # Exponential decay rate for the second moment estimates\n",
        "        epsilon = self.config[\"eps\"]  # Small constant to avoid division by zero\n",
        "        batch_size = self.config[\"batch_size\"]\n",
        "        learning_rate = self.config[\"learning_rate\"]\n",
        "        weight_decay = self.config[\"weight_decay\"]\n",
        "\n",
        "        # Training loop over epochs\n",
        "        for epoch in range(self.config[\"epochs\"]):\n",
        "            # Process data in mini-batches\n",
        "            for batch_start in range(0, len(training_data), batch_size):\n",
        "                batch_data = training_data[batch_start:batch_start + batch_size]\n",
        "                batch_labels = training_labels[batch_start:batch_start + batch_size]\n",
        "\n",
        "                # Initialize gradients for weights and biases\n",
        "                weight_gradients = [np.zeros_like(weight) for weight in self.weights]\n",
        "                bias_gradients = [np.zeros_like(bias) for bias in self.biases]\n",
        "\n",
        "                # Compute gradients for each sample in the batch\n",
        "                for sample_index in range(len(batch_data)):\n",
        "                    # Forward pass\n",
        "                    activations, pre_activations = self.forward_propagate(batch_data[sample_index])\n",
        "                    # Backward pass\n",
        "                    sample_weight_gradients, sample_bias_gradients = self.compute_gradients(activations, pre_activations, batch_labels[sample_index], batch_data[sample_index])\n",
        "\n",
        "                    # Accumulate gradients\n",
        "                    for layer_index in range(self.total_layers):\n",
        "                        weight_gradients[layer_index] += sample_weight_gradients[layer_index]\n",
        "                        bias_gradients[layer_index] += sample_bias_gradients[layer_index]\n",
        "\n",
        "                # Update first and second moment estimates and parameters\n",
        "                for layer_index in range(self.total_layers):\n",
        "                    # Update first moment estimates (mean)\n",
        "                    first_moment_weights[layer_index] = beta1 * first_moment_weights[layer_index] + (1 - beta1) * weight_gradients[layer_index]\n",
        "                    first_moment_biases[layer_index] = beta1 * first_moment_biases[layer_index] + (1 - beta1) * bias_gradients[layer_index]\n",
        "\n",
        "                    # Update second moment estimates (uncentered variance)\n",
        "                    second_moment_weights[layer_index] = beta2 * second_moment_weights[layer_index] + (1 - beta2) * (weight_gradients[layer_index] ** 2)\n",
        "                    second_moment_biases[layer_index] = beta2 * second_moment_biases[layer_index] + (1 - beta2) * (bias_gradients[layer_index] ** 2)\n",
        "\n",
        "                    # Compute bias-corrected moment estimates\n",
        "                    first_moment_weights_hat[layer_index] = first_moment_weights[layer_index] / (1 - np.power(beta1, epoch + 1))\n",
        "                    first_moment_biases_hat[layer_index] = first_moment_biases[layer_index] / (1 - np.power(beta1, epoch + 1))\n",
        "                    second_moment_weights_hat[layer_index] = second_moment_weights[layer_index] / (1 - np.power(beta2, epoch + 1))\n",
        "                    second_moment_biases_hat[layer_index] = second_moment_biases[layer_index] / (1 - np.power(beta2, epoch + 1))\n",
        "\n",
        "                    # Compute NAdam update terms\n",
        "                    weight_update_term = (beta1 * first_moment_weights_hat[layer_index] + (1 - beta1) * weight_gradients[layer_index] / (1 - np.power(beta1, epoch + 1)))\n",
        "                    bias_update_term = (beta1 * first_moment_biases_hat[layer_index] + (1 - beta1) * bias_gradients[layer_index] / (1 - np.power(beta1, epoch + 1)))\n",
        "\n",
        "                    # Update weights and biases using NAdam optimization\n",
        "                    self.weights[layer_index] -= (learning_rate / np.sqrt(second_moment_weights_hat[layer_index] + epsilon)) * weight_update_term + weight_decay * self.weights[layer_index]\n",
        "                    self.biases[layer_index] -= (learning_rate / np.sqrt(second_moment_biases_hat[layer_index] + epsilon)) * bias_update_term\n",
        "\n",
        "            # Calculate and log loss at specific intervals\n",
        "            if (self.config[\"epochs\"] == 10 and epoch % 2 == 1) or self.config[\"epochs\"] == 5:\n",
        "                self.calculate_loss(training_data, training_labels, validation_data, validation_labels, epoch)\n",
        "\n",
        "    def gradient_descent(self,x_train_data, y_train_data, x_validation_data, y_validation_data):\n",
        "        if(self.config[\"optimizer\"] == \"sgd\"):\n",
        "            self.stochastic_gradient_descent(x_train_data, y_train_data, x_validation_data, y_validation_data)\n",
        "        elif(self.config[\"optimizer\"] == \"momentum\"):\n",
        "            self.momentum_based_gradient_descent(x_train_data, y_train_data, x_validation_data, y_validation_data)\n",
        "        elif(self.config[\"optimizer\"] == \"nestrov\"):\n",
        "            self.nesterov_gradient_descent(x_train_data, y_train_data, x_validation_data, y_validation_data)\n",
        "        elif(self.config[\"optimizer\"] == \"rmsprop\"):\n",
        "            self.rmsprop_optimization(x_train_data, y_train_data, x_validation_data, y_validation_data)\n",
        "        elif(self.config[\"optimizer\"] == \"adam\"):\n",
        "            self.adam_optimization(x_train_data, y_train_data, x_validation_data, y_validation_data)\n",
        "        elif(self.config[\"optimizer\"] == \"nadam\"):\n",
        "            self.nadam_optimization(x_train_data, y_train_data, x_validation_data, y_validation_data)\n",
        "\n",
        "    # Method to calculate training and validation loss and accuracy\n",
        "    def compute_loss_and_accuracy(self, train_data, train_labels, validation_data, validation_labels, epoch=0):\n",
        "        train_correct = 0\n",
        "        train_loss = 0\n",
        "        validation_correct = 0\n",
        "        validation_loss = 0\n",
        "        epsilon = 1e-10  # Small constant to avoid log(0)\n",
        "\n",
        "        # Calculate training loss and accuracy\n",
        "        for i in range(len(train_data)):\n",
        "            activations, _ = self.forward_propagate(train_data[i])\n",
        "            predicted_class = np.argmax(activations[self.total_layers - 1])\n",
        "            true_class = train_labels[i]\n",
        "\n",
        "            if predicted_class == true_class:\n",
        "                train_correct += 1\n",
        "\n",
        "            if self.config[\"loss\"] == \"cross_entropy\":\n",
        "                log_value = max(activations[self.total_layers - 1][true_class, 0], epsilon)\n",
        "                train_loss += -math.log10(log_value)\n",
        "            elif self.config[\"loss\"] == \"mean_squared_error\":\n",
        "                one_hot_label = np.zeros((10, 1))\n",
        "                one_hot_label[true_class] = 1\n",
        "                train_loss += np.sum((activations[self.total_layers - 1] - one_hot_label) ** 2)\n",
        "\n",
        "        # Calculate validation loss and accuracy\n",
        "        for i in range(len(validation_data)):\n",
        "            activations, _ = self.forward_propagate(validation_data[i])\n",
        "            predicted_class = np.argmax(activations[self.total_layers - 1])\n",
        "            true_class = validation_labels[i]\n",
        "\n",
        "            if predicted_class == true_class:\n",
        "                validation_correct += 1\n",
        "\n",
        "            if self.config[\"loss\"] == \"cross_entropy\":\n",
        "                log_value = max(activations[self.total_layers - 1][true_class, 0], epsilon)\n",
        "                validation_loss += -math.log10(log_value)\n",
        "            elif self.config[\"loss\"] == \"mean_squared_error\":\n",
        "                one_hot_label = np.zeros((10, 1))\n",
        "                one_hot_label[true_class] = 1\n",
        "                validation_loss += np.sum((activations[self.total_layers - 1] - one_hot_label) ** 2)\n",
        "\n",
        "        # Compute accuracy and average loss\n",
        "        train_accuracy = train_correct / len(train_data)\n",
        "        validation_accuracy = validation_correct / len(validation_data)\n",
        "        train_loss /= len(train_data)\n",
        "        validation_loss /= len(validation_data)\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Epoch: {epoch}, Train Accuracy: {train_accuracy}, Train Loss: {train_loss}, \"\n",
        "              f\"Validation Accuracy: {validation_accuracy}, Validation Loss: {validation_loss}\")\n",
        "\n",
        "        # Log results to wandb if applicable\n",
        "        if (self.config[\"epochs\"] == 10 and epoch % 2 == 1) or self.config[\"epochs\"] == 5:\n",
        "            wandb.log({\n",
        "                \"train_accuracy\": train_accuracy,\n",
        "                \"train_loss\": train_loss,\n",
        "                \"val_accuracy\": validation_accuracy,\n",
        "                \"val_loss\": validation_loss,\n",
        "                \"epoch\": epoch\n",
        "            })\n",
        "\n",
        "\n",
        "    # Method to plot confusion matrix\n",
        "    def generate_confusion_matrix(self, test_data, test_labels):\n",
        "        predicted_labels = []\n",
        "        for i in range(len(test_data)):\n",
        "            activations, _ = self.forward_propagate(test_data[i])\n",
        "            predicted_class = np.argmax(activations[self.total_layers - 1])\n",
        "            predicted_labels.append(predicted_class)\n",
        "\n",
        "        # Create confusion matrix\n",
        "        confusion_matrix = np.zeros((10, 10))\n",
        "        for i in range(len(test_labels)):\n",
        "            confusion_matrix[test_labels[i]][predicted_labels[i]] += 1\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        class_names = ['Ankle boot', 'T-shirt/top', 'Dress', 'Pullover', 'Sneaker', 'Sandal', 'Trouser', 'Shirt', 'Coat', 'Bag']\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(confusion_matrix, annot=True, cmap=plt.cm.Blues, fmt='.2f', xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.xlabel('Predicted Class')\n",
        "        plt.ylabel('True Class')\n",
        "        plt.savefig('confusion_matrix.png')\n",
        "        wandb.log({\"Confusion Matrix\": wandb.Image('confusion_matrix.png')})\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    # Method to update model parameters\n",
        "    def update_model_parameters(self, weight_gradients, bias_gradients, learning_rate):\n",
        "        for i in range(self.total_layers):\n",
        "            self.weights[i] -= learning_rate * weight_gradients[i]\n",
        "            self.biases[i] -= learning_rate * bias_gradients[i]\n",
        "\n",
        "\n",
        "    # Activation functions and their derivatives\n",
        "    def sigmoid(self, x):\n",
        "        clipped_x = np.clip(x, -500, 500)  # Avoid overflow\n",
        "        return 1. / (1. + np.exp(-clipped_x))\n",
        "\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
        "\n",
        "\n",
        "    def tanh(self, x):\n",
        "        clipped_x = np.clip(x, -100, 100)  # Avoid overflow\n",
        "        return np.tanh(clipped_x)\n",
        "\n",
        "\n",
        "    def tanh_derivative(self, x):\n",
        "        return 1 - self.tanh(x) ** 2\n",
        "\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "\n",
        "    def relu_derivative(self, x):\n",
        "        return np.where(x > 0, 1, 0)\n",
        "\n",
        "\n",
        "    def softmax(self, x):\n",
        "        max_val = np.max(x)  # For numerical stability\n",
        "        exp_x = np.exp(x - max_val)\n",
        "        return exp_x / np.sum(exp_x, axis=0)\n",
        "\n",
        "\n",
        "    # Method to apply activation function\n",
        "    def apply_activation(self, activation_name, x):\n",
        "        if activation_name == \"sigmoid\":\n",
        "            return self.sigmoid(x)\n",
        "        elif activation_name == \"relu\":\n",
        "            return self.relu(x)\n",
        "        elif activation_name == \"tanh\":\n",
        "            return self.tanh(x)\n",
        "\n",
        "\n",
        "    # Method to compute activation derivative\n",
        "    def compute_activation_derivative(self, activation_name, x):\n",
        "        if activation_name == \"sigmoid\":\n",
        "            return self.sigmoid_derivative(x)\n",
        "        elif activation_name == \"relu\":\n",
        "            return self.relu_derivative(x)\n",
        "        elif activation_name == \"tanh\":\n",
        "            return self.tanh_derivative(x)\n"
      ],
      "metadata": {
        "id": "m7_rogxqPR6v"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define default hyperparameters for the neural network\n",
        "hyperparameter_defaults = {\n",
        "    \"epochs\": 10,\n",
        "    \"hidden_layers\": 3,\n",
        "    \"hl_size\": 128,\n",
        "    \"weight_decay\": 0,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"batch_size\": 32,\n",
        "    \"initialization\": \"xavier\",\n",
        "    \"activation\": \"relu\",\n",
        "    \"loss\": \"cross_entropy\",\n",
        "    \"wandb_project\": \"DL assignment 1\",\n",
        "    \"wandb_entity\": \"\",\n",
        "    \"momentum_beta\": 0.9,\n",
        "    \"rms_beta\": 0.5,\n",
        "    \"beta1\": 0.9,\n",
        "    \"beta2\": 0.9,\n",
        "    \"eps\": 1e-8\n",
        "}\n"
      ],
      "metadata": {
        "id": "mubHxGKIPSk8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "config = h_param_config_defaults\n",
        "loss_type = \"ce\" if config[\"loss\"] == \"cross_entropy\" else \"mse\"\n",
        "run = wandb.init(project=\"temp proj\", name = f\"{config['optimizer']}_hl_{config['hidden_layers']}_hlsize_{config['hl_size']}_bs_{config['batch_size']}_ac_{config['activation']}_init_{config['initialization']}_loss_{loss_type}\", config=config)\n",
        "nn =NeuralNetwork(784, 10, h_param_config_defaults)\n",
        "nn.gradient_descent(x_train_data, y_train_data, x_validation_data, y_validation_data)\n",
        "nn.plot_confusion_matrix( flattened_test_data, y_test_data )\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Assign hyperparameters to a configuration variable\n",
        "configuration = hyperparameter_defaults\n",
        "\n",
        "# Determine the loss type for naming the run\n",
        "loss_type = \"cross_entropy\" if configuration[\"loss\"] == \"cross_entropy\" else \"mean_squared_error\"\n",
        "\n",
        "# Initialize a new wandb run with a descriptive name\n",
        "wandb_run = wandb.init(\n",
        "    project=\"DeepLearning_Assignment1\",\n",
        "    name=f\"{configuration['optimizer']}_hidden_layers_{configuration['hidden_layers']}_\"\n",
        "         f\"hl_size_{configuration['hl_size']}_batch_size_{configuration['batch_size']}_\"\n",
        "         f\"activation_{configuration['activation']}_initialization_{configuration['initialization']}_\"\n",
        "         f\"loss_{loss_type}\",\n",
        "    config=configuration\n",
        ")\n",
        "\n",
        "# Initialize the neural network\n",
        "neural_network = NeuralNetwork(\n",
        "    input_neurons=784,  # Input layer size (e.g., for MNIST images)\n",
        "    output_neurons=10,  # Output layer size (e.g., for 10 classes)\n",
        "    config=hyperparameter_defaults\n",
        ")\n",
        "\n",
        "# Train the neural network using gradient descent\n",
        "neural_network.gradient_descent(\n",
        "    training_data=x_train_final,  # Changed from x_train_data to x_train_final\n",
        "    training_labels=y_train_final,  # Changed from y_train_data to y_train_final\n",
        "    validation_data=x_validation_final,  # Changed from x_validation_data to x_validation_final\n",
        "    validation_labels=y_validation_final  # Changed from y_validation_data to y_validation_final\n",
        ")\n",
        "\n",
        "# Generate and plot the confusion matrix for test data\n",
        "neural_network.generate_confusion_matrix(  # Changed to generate_confusion_matrix\n",
        "    test_data=x_test_processed,  # Changed from flattened_test_data to x_test_processed\n",
        "    test_labels=test_labels  # Changed from y_test_data to test_labels\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "b0Ql7jQpPSqM",
        "outputId": "8b3048a5-f360-4bda-e4e3-c2e0ff73e63a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NeuralNetwork' object has no attribute 'input_layer_neurons'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-4615f74bfe7d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Train the neural network using gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# The gradient_descent method expects positional arguments, not keyword arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m neural_network.gradient_descent(\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mx_train_final\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Pass training data as a positional argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0my_train_final\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Pass training labels as a positional argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-2eb67b595445>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(self, x_train_data, y_train_data, x_validation_data, y_validation_data)\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmsprop_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_validation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madam_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_validation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nadam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnadam_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_validation_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-2eb67b595445>\u001b[0m in \u001b[0;36madam_optimization\u001b[0;34m(self, training_data, training_labels, validation_data, validation_labels)\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msample_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                     \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m                     \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0msample_weight_gradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_bias_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-2eb67b595445>\u001b[0m in \u001b[0;36mforward_propagate\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlayer_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;31m# Compute pre-activation for the input layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mpre_activations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0;31m# Apply activation function to get the activation for the input layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"activation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_activations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'input_layer_neurons'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GDa5NPeoPSs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ymJ7DC2YPSvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WxU5oJUSPSyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXUoZEGQPS09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQugKrL6PS38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m4bPIeutPS7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ThdvCYakPTCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sD0AToQFPTFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9qfXDuPiPTI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "import wandb\n",
        "\n",
        "class CustomNeuralNetwork:\n",
        "    def __init__(self, input_size, output_size, settings):\n",
        "        # Initialize network architecture\n",
        "        self.num_hidden_layers = settings[\"num_hidden_layers\"]\n",
        "        self.hidden_layer_size = settings[\"hidden_layer_size\"]\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.total_layers = self.num_hidden_layers + 1\n",
        "        self.settings = settings\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Weight initialization\n",
        "        if settings[\"weight_init\"] == \"random\":\n",
        "            self._initialize_random_weights()\n",
        "        elif settings[\"weight_init\"] == \"xavier\":\n",
        "            self._initialize_xavier_weights()\n",
        "\n",
        "    def _initialize_random_weights(self):\n",
        "        # Random initialization with small values\n",
        "        for layer in range(self.total_layers):\n",
        "            if layer == 0:\n",
        "                w = np.random.randn(self.hidden_layer_size, self.input_size) * 0.01\n",
        "                b = np.random.randn(self.hidden_layer_size, 1) * 0.01\n",
        "            elif layer == self.total_layers - 1:\n",
        "                w = np.random.randn(self.output_size, self.hidden_layer_size) * 0.01\n",
        "                b = np.random.randn(self.output_size, 1) * 0.01\n",
        "            else:\n",
        "                w = np.random.randn(self.hidden_layer_size, self.hidden_layer_size) * 0.01\n",
        "                b = np.random.randn(self.hidden_layer_size, 1) * 0.01\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "\n",
        "    def _initialize_xavier_weights(self):\n",
        "        # Xavier/Glorot initialization\n",
        "        for layer in range(self.total_layers):\n",
        "            if layer == 0:\n",
        "                scale = np.sqrt(2.0 / (self.hidden_layer_size + self.input_size))\n",
        "                w = np.random.randn(self.hidden_layer_size, self.input_size) * scale\n",
        "                b = np.zeros((self.hidden_layer_size, 1))\n",
        "            elif layer == self.total_layers - 1:\n",
        "                scale = np.sqrt(2.0 / (self.hidden_layer_size + self.output_size))\n",
        "                w = np.random.randn(self.output_size, self.hidden_layer_size) * scale\n",
        "                b = np.zeros((self.output_size, 1))\n",
        "            else:\n",
        "                scale = np.sqrt(2.0 / (self.hidden_layer_size + self.hidden_layer_size))\n",
        "                w = np.random.randn(self.hidden_layer_size, self.hidden_layer_size) * scale\n",
        "                b = np.zeros((self.hidden_layer_size, 1))\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "\n",
        "    def forward_pass(self, input_data):\n",
        "        # Perform forward propagation\n",
        "        activations = [None] * self.total_layers\n",
        "        pre_activations = [None] * self.total_layers\n",
        "\n",
        "        for layer in range(self.total_layers):\n",
        "            if layer == 0:\n",
        "                pre_activations[layer] = np.dot(self.weights[layer], input_data.reshape(-1, 1)) + self.biases[layer]\n",
        "                activations[layer] = self._apply_activation(self.settings[\"activation\"], pre_activations[layer])\n",
        "            elif layer == self.total_layers - 1:\n",
        "                pre_activations[layer] = np.dot(self.weights[layer], activations[layer - 1]) + self.biases[layer]\n",
        "                activations[layer] = self._apply_softmax(pre_activations[layer])\n",
        "            else:\n",
        "                pre_activations[layer] = np.dot(self.weights[layer], activations[layer - 1]) + self.biases[layer]\n",
        "                activations[layer] = self._apply_activation(self.settings[\"activation\"], pre_activations[layer])\n",
        "\n",
        "        return activations, pre_activations\n",
        "\n",
        "    def backward_pass(self, activations, pre_activations, true_label, input_data):\n",
        "        # Perform backpropagation\n",
        "        grad_weights = [None] * self.total_layers\n",
        "        grad_biases = [None] * self.total_layers\n",
        "        true_label_vector = np.zeros((self.output_size, 1))\n",
        "        true_label_vector[true_label] = 1\n",
        "\n",
        "        # Output layer gradient\n",
        "        if self.settings[\"loss\"] == \"cross_entropy\":\n",
        "            grad_a = -(true_label_vector - activations[-1])\n",
        "        elif self.settings[\"loss\"] == \"mean_squared_error\":\n",
        "            grad_a = (activations[-1] - true_label_vector) * activations[-1] * (1 - activations[-1])\n",
        "\n",
        "        for layer in range(self.total_layers - 1, -1, -1):\n",
        "            if layer == 0:\n",
        "                grad_weights[layer] = np.dot(grad_a, input_data.reshape(1, -1))\n",
        "            else:\n",
        "                grad_weights[layer] = np.dot(grad_a, activations[layer - 1].T)\n",
        "            grad_biases[layer] = np.copy(grad_a)\n",
        "\n",
        "            if layer > 0:\n",
        "                grad_h = np.dot(self.weights[layer].T, grad_a)\n",
        "                grad_a = grad_h * self._apply_activation_derivative(self.settings[\"activation\"], pre_activations[layer - 1])\n",
        "\n",
        "        return grad_weights, grad_biases\n",
        "\n",
        "    def train(self, train_data, train_labels, val_data, val_labels):\n",
        "        # Train the network using the specified optimizer\n",
        "        if self.settings[\"optimizer\"] == \"sgd\":\n",
        "            self._stochastic_gradient_descent(train_data, train_labels, val_data, val_labels)\n",
        "        elif self.settings[\"optimizer\"] == \"adam\":\n",
        "            self._adam_optimizer(train_data, train_labels, val_data, val_labels)\n",
        "        elif self.settings[\"optimizer\"] == \"gradient_descent\":\n",
        "            self._gradient_descent(train_data, train_labels, val_data, val_labels)\n",
        "        elif self.settings[\"optimizer\"] == \"rmsprop\":\n",
        "            self._rmsprop_optimizer(train_data, train_labels, val_data, val_labels)\n",
        "        elif self.settings[\"optimizer\"] == \"nadam\":\n",
        "            self._nadam_optimizer(train_data, train_labels, val_data, val_labels)\n",
        "\n",
        "    def _nadam_optimizer(self, train_data, train_labels, val_data, val_labels):\n",
        "        # Nadam optimizer implementation\n",
        "        batch_size = self.settings[\"batch_size\"]\n",
        "        num_epochs = self.settings[\"epochs\"]\n",
        "        learning_rate = self.settings[\"learning_rate\"]\n",
        "        weight_decay = self.settings[\"weight_decay\"]\n",
        "        beta1 = self.settings[\"beta1\"]\n",
        "        beta2 = self.settings[\"beta2\"]\n",
        "        epsilon = self.settings[\"epsilon\"]\n",
        "\n",
        "        # Initialize momentum and squared gradient moving averages\n",
        "        m_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        m_biases = [np.zeros_like(b) for b in self.biases]\n",
        "        v_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        v_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            for i in range(0, len(train_data), batch_size):\n",
        "                batch_data = train_data[i:i + batch_size]\n",
        "                batch_labels = train_labels[i:i + batch_size]\n",
        "\n",
        "                grad_weights = [np.zeros_like(w) for w in self.weights]\n",
        "                grad_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "                # Compute gradients for the batch\n",
        "                for j in range(len(batch_data)):\n",
        "                    activations, pre_activations = self.forward_pass(batch_data[j])\n",
        "                    dw, db = self.backward_pass(activations, pre_activations, batch_labels[j], batch_data[j])\n",
        "                    for k in range(self.total_layers):\n",
        "                        grad_weights[k] += dw[k]\n",
        "                        grad_biases[k] += db[k]\n",
        "\n",
        "                # Update momentum and squared gradient moving averages\n",
        "                for k in range(self.total_layers):\n",
        "                    m_weights[k] = beta1 * m_weights[k] + (1 - beta1) * grad_weights[k]\n",
        "                    m_biases[k] = beta1 * m_biases[k] + (1 - beta1) * grad_biases[k]\n",
        "                    v_weights[k] = beta2 * v_weights[k] + (1 - beta2) * (grad_weights[k] ** 2)\n",
        "                    v_biases[k] = beta2 * v_biases[k] + (1 - beta2) * (grad_biases[k] ** 2)\n",
        "\n",
        "                    # Bias-corrected estimates\n",
        "                    m_weights_hat = m_weights[k] / (1 - beta1 ** (epoch + 1))\n",
        "                    m_biases_hat = m_biases[k] / (1 - beta1 ** (epoch + 1))\n",
        "                    v_weights_hat = v_weights[k] / (1 - beta2 ** (epoch + 1))\n",
        "                    v_biases_hat = v_biases[k] / (1 - beta2 ** (epoch + 1))\n",
        "\n",
        "                    # Nesterov momentum + Adam update\n",
        "                    self.weights[k] -= (learning_rate / (np.sqrt(v_weights_hat) + epsilon)) * (\n",
        "                        beta1 * m_weights_hat + (1 - beta1) * grad_weights[k] / (1 - beta1 ** (epoch + 1)))\n",
        "                    self.biases[k] -= (learning_rate / (np.sqrt(v_biases_hat) + epsilon)) * (\n",
        "                        beta1 * m_biases_hat + (1 - beta1) * grad_biases[k] / (1 - beta1 ** (epoch + 1)))\n",
        "\n",
        "            # Evaluate performance\n",
        "            if (epoch + 1) % 2 == 0:\n",
        "                self._evaluate_performance(train_data, train_labels, val_data, val_labels, epoch + 1)\n",
        "\n",
        "    def _rmsprop_optimizer(self, train_data, train_labels, val_data, val_labels):\n",
        "        # RMSProp optimizer implementation\n",
        "        batch_size = self.settings[\"batch_size\"]\n",
        "        num_epochs = self.settings[\"epochs\"]\n",
        "        learning_rate = self.settings[\"learning_rate\"]\n",
        "        weight_decay = self.settings[\"weight_decay\"]\n",
        "        beta = self.settings[\"rms_beta\"]\n",
        "        epsilon = 1e-8\n",
        "\n",
        "        # Initialize moving average of squared gradients\n",
        "        v_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        v_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            for i in range(0, len(train_data), batch_size):\n",
        "                batch_data = train_data[i:i + batch_size]\n",
        "                batch_labels = train_labels[i:i + batch_size]\n",
        "\n",
        "                grad_weights = [np.zeros_like(w) for w in self.weights]\n",
        "                grad_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "                # Compute gradients for the batch\n",
        "                for j in range(len(batch_data)):\n",
        "                    activations, pre_activations = self.forward_pass(batch_data[j])\n",
        "                    dw, db = self.backward_pass(activations, pre_activations, batch_labels[j], batch_data[j])\n",
        "                    for k in range(self.total_layers):\n",
        "                        grad_weights[k] += dw[k]\n",
        "                        grad_biases[k] += db[k]\n",
        "\n",
        "                # Update moving average of squared gradients and weights\n",
        "                for k in range(self.total_layers):\n",
        "                    v_weights[k] = beta * v_weights[k] + (1 - beta) * (grad_weights[k] ** 2)\n",
        "                    v_biases[k] = beta * v_biases[k] + (1 - beta) * (grad_biases[k] ** 2)\n",
        "\n",
        "                    self.weights[k] -= learning_rate * (grad_weights[k] / (np.sqrt(v_weights[k]) + epsilon)) + weight_decay * self.weights[k]\n",
        "                    self.biases[k] -= learning_rate * (grad_biases[k] / (np.sqrt(v_biases[k]) + epsilon))\n",
        "\n",
        "            # Evaluate performance\n",
        "            if (epoch + 1) % 2 == 0:\n",
        "                self._evaluate_performance(train_data, train_labels, val_data, val_labels, epoch + 1)\n",
        "\n",
        "    def _gradient_descent(self, train_data, train_labels, val_data, val_labels):\n",
        "        # Vanilla Gradient Descent implementation\n",
        "        num_epochs = self.settings[\"epochs\"]\n",
        "        learning_rate = self.settings[\"learning_rate\"]\n",
        "        weight_decay = self.settings[\"weight_decay\"]\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            grad_weights = [np.zeros_like(w) for w in self.weights]\n",
        "            grad_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "            # Compute gradients for the entire dataset\n",
        "            for j in range(len(train_data)):\n",
        "                activations, pre_activations = self.forward_pass(train_data[j])\n",
        "                dw, db = self.backward_pass(activations, pre_activations, train_labels[j], train_data[j])\n",
        "                for k in range(self.total_layers):\n",
        "                    grad_weights[k] += dw[k]\n",
        "                    grad_biases[k] += db[k]\n",
        "\n",
        "            # Update weights and biases\n",
        "            for k in range(self.total_layers):\n",
        "                self.weights[k] -= learning_rate * (grad_weights[k] / len(train_data) + weight_decay * self.weights[k])\n",
        "                self.biases[k] -= learning_rate * (grad_biases[k] / len(train_data))\n",
        "\n",
        "            # Evaluate performance\n",
        "            if (epoch + 1) % 2 == 0:\n",
        "                self._evaluate_performance(train_data, train_labels, val_data, val_labels, epoch + 1)\n",
        "\n",
        "    def _stochastic_gradient_descent(self, train_data, train_labels, val_data, val_labels):\n",
        "        # SGD implementation\n",
        "        batch_size = self.settings[\"batch_size\"]\n",
        "        num_epochs = self.settings[\"epochs\"]\n",
        "        learning_rate = self.settings[\"learning_rate\"]\n",
        "        weight_decay = self.settings[\"weight_decay\"]\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            for i in range(0, len(train_data), batch_size):\n",
        "                batch_data = train_data[i:i + batch_size]\n",
        "                batch_labels = train_labels[i:i + batch_size]\n",
        "\n",
        "                grad_weights = [np.zeros_like(w) for w in self.weights]\n",
        "                grad_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "                for j in range(len(batch_data)):\n",
        "                    activations, pre_activations = self.forward_pass(batch_data[j])\n",
        "                    dw, db = self.backward_pass(activations, pre_activations, batch_labels[j], batch_data[j])\n",
        "                    for k in range(self.total_layers):\n",
        "                        grad_weights[k] += dw[k]\n",
        "                        grad_biases[k] += db[k]\n",
        "\n",
        "                for k in range(self.total_layers):\n",
        "                    self.weights[k] -= learning_rate * (grad_weights[k] / batch_size + weight_decay * self.weights[k])\n",
        "                    self.biases[k] -= learning_rate * (grad_biases[k] / batch_size)\n",
        "\n",
        "            if (epoch + 1) % 2 == 0:\n",
        "                self._evaluate_performance(train_data, train_labels, val_data, val_labels, epoch + 1)\n",
        "\n",
        "    def _adam_optimizer(self, train_data, train_labels, val_data, val_labels):\n",
        "        # Adam optimizer implementation\n",
        "        batch_size = self.settings[\"batch_size\"]\n",
        "        num_epochs = self.settings[\"epochs\"]\n",
        "        learning_rate = self.settings[\"learning_rate\"]\n",
        "        weight_decay = self.settings[\"weight_decay\"]\n",
        "        beta1 = self.settings[\"beta1\"]\n",
        "        beta2 = self.settings[\"beta2\"]\n",
        "        epsilon = self.settings[\"epsilon\"]\n",
        "\n",
        "        m_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        m_biases = [np.zeros_like(b) for b in self.biases]\n",
        "        v_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        v_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            for i in range(0, len(train_data), batch_size):\n",
        "                batch_data = train_data[i:i + batch_size]\n",
        "                batch_labels = train_labels[i:i + batch_size]\n",
        "\n",
        "                grad_weights = [np.zeros_like(w) for w in self.weights]\n",
        "                grad_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "                for j in range(len(batch_data)):\n",
        "                    activations, pre_activations = self.forward_pass(batch_data[j])\n",
        "                    dw, db = self.backward_pass(activations, pre_activations, batch_labels[j], batch_data[j])\n",
        "                    for k in range(self.total_layers):\n",
        "                        grad_weights[k] += dw[k]\n",
        "                        grad_biases[k] += db[k]\n",
        "\n",
        "                for k in range(self.total_layers):\n",
        "                    m_weights[k] = beta1 * m_weights[k] + (1 - beta1) * grad_weights[k]\n",
        "                    m_biases[k] = beta1 * m_biases[k] + (1 - beta1) * grad_biases[k]\n",
        "                    v_weights[k] = beta2 * v_weights[k] + (1 - beta2) * (grad_weights[k] ** 2)\n",
        "                    v_biases[k] = beta2 * v_biases[k] + (1 - beta2) * (grad_biases[k] ** 2)\n",
        "\n",
        "                    m_weights_hat = m_weights[k] / (1 - beta1 ** (epoch + 1))\n",
        "                    m_biases_hat = m_biases[k] / (1 - beta1 ** (epoch + 1))\n",
        "                    v_weights_hat = v_weights[k] / (1 - beta2 ** (epoch + 1))\n",
        "                    v_biases_hat = v_biases[k] / (1 - beta2 ** (epoch + 1))\n",
        "\n",
        "                    self.weights[k] -= learning_rate * m_weights_hat / (np.sqrt(v_weights_hat) + epsilon) + weight_decay * self.weights[k]\n",
        "                    self.biases[k] -= learning_rate * m_biases_hat / (np.sqrt(v_biases_hat) + epsilon)\n",
        "\n",
        "            if (epoch + 1) % 2 == 0:\n",
        "                self._evaluate_performance(train_data, train_labels, val_data, val_labels, epoch + 1)\n",
        "\n",
        "    def _compute_loss_and_accuracy(self, data, labels):\n",
        "        # Compute loss and accuracy for a dataset\n",
        "        loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        for i in range(len(data)):\n",
        "            activations, _ = self.forward_pass(data[i])\n",
        "            predicted_class = np.argmax(activations[-1])\n",
        "            true_class = labels[i]\n",
        "\n",
        "            if predicted_class == true_class:\n",
        "                correct += 1\n",
        "\n",
        "            if self.settings[\"loss\"] == \"cross_entropy\":\n",
        "                loss += -np.log(activations[-1][true_class] + 1e-10)\n",
        "            elif self.settings[\"loss\"] == \"mean_squared_error\":\n",
        "                true_label_vector = np.zeros((self.output_size, 1))\n",
        "                true_label_vector[true_class] = 1\n",
        "                loss += np.sum((activations[-1] - true_label_vector) ** 2)\n",
        "\n",
        "        accuracy = correct / len(data)\n",
        "        loss = loss / len(data)\n",
        "        return loss.item(), accuracy  # Convert loss to a scalar value using .item()\n",
        "\n",
        "    def _evaluate_performance(self, train_data, train_labels, val_data, val_labels, epoch):\n",
        "        train_loss, train_accuracy = self._compute_loss_and_accuracy(train_data, train_labels)\n",
        "        val_loss, val_accuracy = self._compute_loss_and_accuracy(val_data, val_labels)\n",
        "\n",
        "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_accuracy:.4f}, \"\n",
        "              f\"Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n",
        "\n",
        "        if self.settings[\"use_wandb\"]:\n",
        "            wandb.log({\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_accuracy\": train_accuracy,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"val_accuracy\": val_accuracy,\n",
        "                \"epoch\": epoch\n",
        "            })\n",
        "\n",
        "        def _apply_activation(self, activation, x):\n",
        "            # Apply activation function\n",
        "            if activation == \"sigmoid\":\n",
        "                return 1 / (1 + np.exp(-x))\n",
        "            elif activation == \"relu\":\n",
        "                return np.maximum(0, x)\n",
        "            elif activation == \"tanh\":\n",
        "                return np.tanh(x)\n",
        "\n",
        "    def _apply_activation_derivative(self, activation, x):\n",
        "        # Apply activation derivative\n",
        "        if activation == \"sigmoid\":\n",
        "            return self._apply_activation(\"sigmoid\", x) * (1 - self._apply_activation(\"sigmoid\", x))\n",
        "        elif activation == \"relu\":\n",
        "            return np.where(x > 0, 1, 0)\n",
        "        elif activation == \"tanh\":\n",
        "            return 1 - np.tanh(x) ** 2\n",
        "\n",
        "    def _apply_softmax(self, x):\n",
        "        # Apply softmax function\n",
        "        exp_x = np.exp(x - np.max(x))\n",
        "        return exp_x / np.sum(exp_x, axis=0)"
      ],
      "metadata": {
        "id": "9a0SHkjFRj1s"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define configuration\n",
        "config = {\n",
        "    \"epochs\": 10,\n",
        "    \"num_hidden_layers\": 3,\n",
        "    \"hidden_layer_size\": 128,\n",
        "    \"weight_decay\": 0,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"batch_size\": 32,\n",
        "    \"weight_init\": \"xavier\",\n",
        "    \"activation\": \"relu\",\n",
        "    \"loss\": \"cross_entropy\",\n",
        "    \"wandb_project\": \"DeepLearning_Assignment1\",\n",
        "    \"wandb_entity\": \"\",\n",
        "    \"momentum_beta\": 0.9,\n",
        "    \"rms_beta\": 0.5,\n",
        "    \"beta1\": 0.9,\n",
        "    \"beta2\": 0.9,\n",
        "    \"eps\": 1e-8\n",
        "}\n",
        "\n",
        "# Initialize wandb\n",
        "loss_type = \"ce\" if config[\"loss\"] == \"cross_entropy\" else \"mse\"\n",
        "run = wandb.init(\n",
        "    project=config[\"wandb_project\"],\n",
        "    name=f\"{config['optimizer']}_hl_{config['num_hidden_layers']}_hlsize_{config['hidden_layer_size']}_bs_{config['batch_size']}_ac_{config['activation']}_init_{config['weight_init']}_loss_{loss_type}\",\n",
        "    config=config\n",
        ")\n",
        "\n",
        "# Import fetch_openml and other necessary modules\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load and preprocess data\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "X, y = mnist.data, mnist.target\n",
        "X = X / 255.0  # Normalize\n",
        "lb = LabelBinarizer()\n",
        "y = lb.fit_transform(y)  # One-hot encoding\n",
        "x_train_final, x_validation_final, y_train_final, y_validation_final = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the model\n",
        "nn = CustomNeuralNetwork(784, 10, config)\n",
        "nn._gradient_descent(x_train_final, y_train_final, x_validation_final, y_validation_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "n2GN1LMGO175",
        "outputId": "1a8088d5-4d36-4c16-fc0c-5b2dbebb2e96"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3c559abf0978>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Initialize and train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_validation_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-bf0f917f64f2>\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(self, train_data, train_labels, val_data, val_labels)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;31m# Compute gradients for the entire dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_activations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define configuration\n",
        "config = {\n",
        "     \"epochs\":10,\n",
        "    \"num_hidden_layers\":3,\n",
        "    \"hidden_layer_size\":128,\n",
        "    \"weight_decay\":0,\n",
        "    \"learning_rate\":0.0001,\n",
        "    \"optimizer\":\"adam\",\n",
        "    \"batch_size\":32,\n",
        "    \"weight_init\":\"xavier\",\n",
        "    \"activation\":\"relu\",\n",
        "    \"loss\":\"cross_entropy\",\n",
        "    \"wandb_project\":\"DeepLearning_Assignment1\",\n",
        "    \"wandb_entity\":\"\",\n",
        "    \"momentum_beta\":0.9,\n",
        "    \"rms_beta\":0.5,\n",
        "    \"beta1\":0.9,\n",
        "    \"beta2\":0.9,\n",
        "    \"eps\":1e-8\n",
        "}"
      ],
      "metadata": {
        "id": "zqKt-83PRkj0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_type = \"ce\" if config[\"loss\"] == \"cross_entropy\" else \"mse\"\n",
        "run = wandb.init(project=\"DeepLearning_Assignment1\", name = f\"{config['optimizer']}_hl_{config['num_hidden_layers']}_hlsize_{config['hidden_layer_size']}_bs_{config['batch_size']}_ac_{config['_apply_activation']}_init_{config['weight_init']}_loss_{loss_type}\", config=config)\n",
        "nn = CustomNeuralNetwork(784, 10, config)\n",
        "nn._gradient_descent(x_train_final, y_train_final, x_validation_final, y_validation_final)\n",
        "#nn.plot_confusion_matrix( flattened_test_data, y_test_data )\n",
        "\n",
        "\n",
        "name = f\"{config['optimizer']}_hl_{config['num_hidden_layers']}_hlsize_{config['hidden_layer_size']}_bs_{config['batch_size']}_ac_{config['activation']}_init_{config['weight_init']}_loss_{loss_type}\""
      ],
      "metadata": {
        "id": "TYh9ou3ySZXl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "69182298-0166-4a6b-b7a7-4b25e1971ebb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'_apply_activation'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-42348b5587ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloss_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ce\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cross_entropy\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"DeepLearning_Assignment1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{config['optimizer']}_hl_{config['num_hidden_layers']}_hlsize_{config['hidden_layer_size']}_bs_{config['batch_size']}_ac_{config['_apply_activation']}_init_{config['weight_init']}_loss_{loss_type}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_validation_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_validation_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#nn.plot_confusion_matrix( flattened_test_data, y_test_data )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: '_apply_activation'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_params = {\n",
        "    'method' : 'bayes',\n",
        "    'name'   : 'sweep-2',\n",
        "    'metric' : {\n",
        "        'goal' : 'maximize',\n",
        "        'name' : 'train_accuracy',\n",
        "    },\n",
        "    'parameters' : {\n",
        "        'epochs':{'values' : [5,10]},\n",
        "        'hidden_layers':{'values' : [3,4,5]},\n",
        "        'hl_size':{'values':[32,64,128]},\n",
        "        'weight_decay':{'values' : [0, 0.0005, 0.5] } ,\n",
        "        'learning_rate':{'values' : [0.0001,0.001]},\n",
        "        'optimizer':{'values':['sgd','momentum','nestrov','rmsprop','adam', 'nadam']},\n",
        "        'batch_size':{'values' : [16,32,64]},\n",
        "        'initialization':{'values': ['random','xavier']},\n",
        "        'activation':{'values' : ['sigmoid','tanh','relu']}\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep=sweep_params, project=\"DeepLearning_Assignment1\")\n",
        "print(sweep_id)\n",
        "def train():\n",
        "    run = wandb.init(project=\"DeepLearning_Assignment1\")\n",
        "    config = wandb.config\n",
        "    n_network = CustomNeuralNetwork(784, 10, config)\n",
        "    n_network.gradient_descent(x_train_final, y_train_final, x_validation_final, y_validation_final)\n",
        "wandb.agent(sweep_id, function=train, count=50)"
      ],
      "metadata": {
        "id": "P1wOAwN3R8gT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d5eb09d5-6837-4501-e99f-580562af7901"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: e1j0sjtb\n",
            "Sweep URL: https://wandb.ai/m_dhamu2908/DeepLearning_Assignment1/sweeps/e1j0sjtb\n",
            "e1j0sjtb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x1bxoa30 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thl_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitialization: xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DeepLearning_Assignment1' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-12 (_run_job):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "    self._function()\n",
            "  File \"<ipython-input-10-e09f24eacce3>\", line 26, in train\n",
            "  File \"<ipython-input-8-a089c5d4d0dc>\", line 10, in __init__\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_config.py\", line 130, in __getitem__\n",
            "    return self._items[key]\n",
            "           ~~~~~~~~~~~^^^^^\n",
            "KeyError: 'num_hidden_layers'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 311, in _run_job\n",
            "    wandb.finish(exit_code=1)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 4130, in finish\n",
            "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 449, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 391, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 2106, in finish\n",
            "    return self._finish(exit_code)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 2113, in _finish\n",
            "    with telemetry.context(run=self) as tel:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
            "    self._run._telemetry_callback(self._obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 749, in _telemetry_callback\n",
            "    self._telemetry_flush()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 762, in _telemetry_flush\n",
            "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_shared.py\", line 60, in _publish_telemetry\n",
            "    self._publish(rec)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_sock.py\", line 39, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 174, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 154, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 151, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6yziergb with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thl_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitialization: random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DeepLearning_Assignment1' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-13 (_run_job):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "    self._function()\n",
            "  File \"<ipython-input-10-e09f24eacce3>\", line 24, in train\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 1482, in init\n",
            "    wandb._sentry.reraise(e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/analytics/sentry.py\", line 156, in reraise\n",
            "    raise exc.with_traceback(sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 1468, in init\n",
            "    return wi.init(run_settings, run_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 779, in init\n",
            "    with telemetry.context() as tel:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
            "    self._run._telemetry_callback(self._obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 749, in _telemetry_callback\n",
            "    self._telemetry_flush()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 762, in _telemetry_flush\n",
            "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_shared.py\", line 60, in _publish_telemetry\n",
            "    self._publish(rec)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_sock.py\", line 39, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 174, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 154, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 151, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 311, in _run_job\n",
            "    wandb.finish(exit_code=1)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 4130, in finish\n",
            "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 449, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 391, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 2106, in finish\n",
            "    return self._finish(exit_code)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 2113, in _finish\n",
            "    with telemetry.context(run=self) as tel:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
            "    self._run._telemetry_callback(self._obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 749, in _telemetry_callback\n",
            "    self._telemetry_flush()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 762, in _telemetry_flush\n",
            "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_shared.py\", line 60, in _publish_telemetry\n",
            "    self._publish(rec)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_sock.py\", line 39, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 174, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 154, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 151, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x5rb3yyf with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thl_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitialization: xavier\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DeepLearning_Assignment1' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-14 (_run_job):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "    self._function()\n",
            "  File \"<ipython-input-10-e09f24eacce3>\", line 24, in train\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 1482, in init\n",
            "    wandb._sentry.reraise(e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/analytics/sentry.py\", line 156, in reraise\n",
            "    raise exc.with_traceback(sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 1468, in init\n",
            "    return wi.init(run_settings, run_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 779, in init\n",
            "    with telemetry.context() as tel:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
            "    self._run._telemetry_callback(self._obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 749, in _telemetry_callback\n",
            "    self._telemetry_flush()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 762, in _telemetry_flush\n",
            "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_shared.py\", line 60, in _publish_telemetry\n",
            "    self._publish(rec)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_sock.py\", line 39, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 174, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 154, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 151, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 311, in _run_job\n",
            "    wandb.finish(exit_code=1)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 4130, in finish\n",
            "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 449, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 391, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 2106, in finish\n",
            "    return self._finish(exit_code)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 2113, in _finish\n",
            "    with telemetry.context(run=self) as tel:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
            "    self._run._telemetry_callback(self._obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 749, in _telemetry_callback\n",
            "    self._telemetry_flush()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 762, in _telemetry_flush\n",
            "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_shared.py\", line 60, in _publish_telemetry\n",
            "    self._publish(rec)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_sock.py\", line 39, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 174, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 154, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 151, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ltund60l with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layers: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thl_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitialization: random\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nestrov\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Ignoring project 'DeepLearning_Assignment1' when running a sweep."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-15 (_run_job):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
            "    self._function()\n",
            "  File \"<ipython-input-10-e09f24eacce3>\", line 24, in train\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 1482, in init\n",
            "    wandb._sentry.reraise(e)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/analytics/sentry.py\", line 156, in reraise\n",
            "    raise exc.with_traceback(sys.exc_info()[2])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 1468, in init\n",
            "    return wi.init(run_settings, run_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\", line 779, in init\n",
            "    with telemetry.context() as tel:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
            "    self._run._telemetry_callback(self._obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 749, in _telemetry_callback\n",
            "    self._telemetry_flush()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 762, in _telemetry_flush\n",
            "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_shared.py\", line 60, in _publish_telemetry\n",
            "    self._publish(rec)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_sock.py\", line 39, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 174, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 154, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 151, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 311, in _run_job\n",
            "    wandb.finish(exit_code=1)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 4130, in finish\n",
            "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 449, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 391, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 2106, in finish\n",
            "    return self._finish(exit_code)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 2113, in _finish\n",
            "    with telemetry.context(run=self) as tel:\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/telemetry.py\", line 42, in __exit__\n",
            "    self._run._telemetry_callback(self._obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 749, in _telemetry_callback\n",
            "    self._telemetry_flush()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\", line 762, in _telemetry_flush\n",
            "    self._backend.interface._publish_telemetry(self._telemetry_obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_shared.py\", line 60, in _publish_telemetry\n",
            "    self._publish(rec)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_sock.py\", line 39, in _publish\n",
            "    self._sock_client.send_record_publish(record)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 174, in send_record_publish\n",
            "    self.send_server_request(server_req)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 154, in send_server_request\n",
            "    self._send_message(msg)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 151, in _send_message\n",
            "    self._sendall_with_error_handle(header + data)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\", line 130, in _sendall_with_error_handle\n",
            "    sent = self._sock.send(data)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x785fdc1fdad0>> (for post_run_cell):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BrokenPipeError",
          "evalue": "[Errno 32] Broken pipe",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m_pause_backend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pausing backend\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resume_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#  noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mpublish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpublish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mpause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauseRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpause\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauseRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_sock.py\u001b[0m in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pb.Record\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_record_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36msend_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mserver_req\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmailbox_slot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mserver_req\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_publish\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_packet_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36msend_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServerRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServerResponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36m_send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<BI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sendall_with_error_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServerRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36m_sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0;31m# sent equal to 0 indicates a closed socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "caJ_2fdXCGie",
        "outputId": "a03e09e0-9305-4437-e70b-de071d4f15eb"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f9fb0e0df10>> (for post_run_cell):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BrokenPipeError",
          "evalue": "[Errno 32] Broken pipe",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m_pause_backend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pausing backend\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_resume_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#  noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mpublish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpublish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mpause\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauseRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_pause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpause\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauseRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResumeRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/interface/interface_sock.py\u001b[0m in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pb.Record\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_record_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36msend_record_publish\u001b[0;34m(self, record)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mserver_req\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmailbox_slot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mserver_req\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_publish\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_packet_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36msend_server_request\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServerRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServerResponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36m_send_message\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<BI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sendall_with_error_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_server_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mServerRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/sock_client.py\u001b[0m in \u001b[0;36m_sendall_with_error_handle\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m                 \u001b[0;31m# sent equal to 0 indicates a closed socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        }
      ]
    }
  ]
}